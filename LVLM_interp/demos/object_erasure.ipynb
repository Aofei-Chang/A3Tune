{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.llava_utils import load_llava_state\n",
    "from methods.blip_utils import load_blip_state\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from methods.algorithms import get_phrase_embedding, generate_mass_edit_hook\n",
    "from methods.utils import coco_img_id_to_name, display_image\n",
    "import torch\n",
    "import random\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "os.chdir(os.environ[\"VL_ROOT_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"llava7b\"\n",
    "lT = 19\n",
    "lI = 21\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"llava7b\":\n",
    "  # Load the LlaVA model\n",
    "  loaded_state = load_llava_state(model_type, train = True)\n",
    "elif model_type == \"blip7b\":\n",
    "  loaded_state = load_blip_state(model_type, train = True)\n",
    "else:\n",
    "  raise Exception(f\"model type {model_type} not supported\")\n",
    "\n",
    "vocabulary, vocab_embeddings, data, execute_model, register_hook, tokenizer, hidden_layer_embedding = loaded_state[\"vocabulary\"], loaded_state[\"vocab_embeddings\"], loaded_state[\"data\"], loaded_state[\"execute_model\"], loaded_state[\"register_hook\"], loaded_state[\"tokenizer\"], loaded_state[\"hidden_layer_embedding\"]\n",
    "\n",
    "id_to_token = dict()\n",
    "for word in vocabulary:\n",
    "  id_to_token[vocabulary[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_img = 562150\n",
    "image_path = os.path.join('./images', coco_img_id_to_name(coco_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = pickle.load(open('./metric/chair.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline caption\n",
    "baseline_caption = execute_model(image_path)\n",
    "baseline_evals = evaluator.compute_hallucinations(coco_img, baseline_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a target object to erase and extract a text embedding for the object\n",
    "\n",
    "text_embeddings = []\n",
    "for caption_word, coco_class in set(baseline_evals[\"mscoco_hallucinated_words\"]):\n",
    "  text_embeddings.append(hidden_layer_embedding(caption_word, layer = lT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook into the model's intermediate activations to linearly edit them\n",
    "\n",
    "if model_type == \"llava7b\":\n",
    "  edit_embeddings_hook = generate_mass_edit_hook(text_embeddings, start_edit_index=35, end_edit_index=611, layer=lT, weight = alpha, minimum_size=576)\n",
    "else:\n",
    "  edit_embeddings_hook = generate_mass_edit_hook(text_embeddings, start_edit_index=0, end_edit_index=32, layer=lT, weight = alpha, minimum_size=32)\n",
    "hook = register_hook(edit_embeddings_hook, lI)\n",
    "\n",
    "# Remember to remove the hook if you want to try another layer!\n",
    "# hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_caption = execute_model(image_path)\n",
    "\n",
    "# Compute the hallucinations\n",
    "new_chair_eval = evaluator.compute_hallucinations(coco_img, new_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(image_path)\n",
    "print(\"==== Baseline ====\")\n",
    "print(baseline_caption)\n",
    "print(baseline_evals)\n",
    "print(\"==== Edited ====\")\n",
    "print(new_caption)\n",
    "print(new_chair_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
