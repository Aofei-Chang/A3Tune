{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327,\n",
       " {'image': 'xmlab1/source.jpg',\n",
       "  'id': 3,\n",
       "  'location': 'Abdomen',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nDoes the picture contain liver?'},\n",
       "   {'from': 'gpt', 'value': 'Yes'}],\n",
       "  'bboxes': [[54.0, 106.0, 30.0, 31.0]]})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "gt_qa_path = \"/data/aofei/hallucination/Slake/data/training_gt_only_bboxes_abd.json\"\n",
    "gt = json.load(open(gt_qa_path, 'r'))\n",
    "len(gt), gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_gpt(gt_qa):\n",
    "    data = []\n",
    "    for item in gt_qa:\n",
    "        if len(item[\"bboxes\"]) >= 2:\n",
    "            continue\n",
    "        data.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"question\": item[\"conversations\"][0]['value'],\n",
    "            \"answers\": item[\"conversations\"][1]['value'],\n",
    "            \"bbox\": item[\"bboxes\"],\n",
    "            \"image\": item[\"image\"]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "prorcessed_gt = pre_process_gpt(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272,\n",
       " {'id': 3,\n",
       "  'question': '<image>\\nDoes the picture contain liver?',\n",
       "  'answers': 'Yes',\n",
       "  'bbox': [[54.0, 106.0, 30.0, 31.0]],\n",
       "  'image': 'xmlab1/source.jpg'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prorcessed_gt), prorcessed_gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def transform_and_normalize_bbox(bbox, image_path):\n",
    "    # Load the image and get its dimensions\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "\n",
    "    # Original coordinates\n",
    "    top_left_x = bbox[0]\n",
    "    top_left_y = bbox[1]\n",
    "    width = bbox[2]\n",
    "    height = bbox[3]\n",
    "\n",
    "    # Calculate bottom-right coordinates\n",
    "    bottom_right_x = top_left_x + width\n",
    "    bottom_right_y = top_left_y + height\n",
    "\n",
    "    # Normalize and round coordinates\n",
    "    normalized_top_left_x = round(top_left_x / image_width, 2)\n",
    "    normalized_top_left_y = round(top_left_y / image_height, 2)\n",
    "    normalized_bottom_right_x = round(bottom_right_x / image_width, 2)\n",
    "    normalized_bottom_right_y = round(bottom_right_y / image_height, 2)\n",
    "\n",
    "    # Return normalized bounding box\n",
    "    normalized_bbox = [\n",
    "        normalized_top_left_x,\n",
    "        normalized_top_left_y,\n",
    "        normalized_bottom_right_x,\n",
    "        normalized_bottom_right_y\n",
    "    ]\n",
    "    return normalized_bbox, (image_width, image_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'question': '<image>\\nDoes the picture contain liver?',\n",
       " 'answers': 'Yes',\n",
       " 'bbox': [[54.0, 106.0, 30.0, 31.0]],\n",
       " 'image': 'xmlab1/source.jpg',\n",
       " 'normalized_bbox': [0.21, 0.41, 0.33, 0.54],\n",
       " 'image_size': (256, 256)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "for item in prorcessed_gt:\n",
    "    image_path = os.path.join(\"/data/aofei/hallucination/Slake/imgs\", item[\"image\"])\n",
    "    item[\"normalized_bbox\"], item[\"image_size\"] = transform_and_normalize_bbox(item[\"bbox\"][0], image_path)\n",
    "\n",
    "prorcessed_gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]='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'\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_vqa_with_gpt4o_in_batches(vqas, batch_size=5):\n",
    "    new_answers = []\n",
    "\n",
    "    # Loop through questions, predictions, and ground truths in batches\n",
    "    for i in tqdm(range(0, len(vqas), batch_size)):\n",
    "        batch_questions = vqas[i:i+batch_size]\n",
    "        \n",
    "        # Create a batch prompt\n",
    "        batch_prompt = batch_questions\n",
    "        full_prompt = \"\"\"\n",
    "        You are helping me generate a dataset of visual question-answer pairs with bounding box information. Each item should have an answer in a visual grounding style, referring to the specific location of the region within the image.\n",
    "\n",
    "        For each data item in the format {'id': <ID>, 'question': <QUESTION>, 'answers': <ANSWER>, 'bbox': <BBOX>, 'image': <IMAGE_PATH>, 'normalized_bbox': <NORMALIZED_BBOX>, 'image_size': <IMAGE_SIZE>}, please provide the answer in the following style:\n",
    "\n",
    "        Describe the location of the object or region using the bounding box coordinates.\n",
    "        Give an assessment of the object based on the answer provided, if applicable.\n",
    "\n",
    "        Example item in the Input list:\n",
    "        {\n",
    "        'id': 3,\n",
    "        'question': \"<image>\\nDoes the liver look normal?\",\n",
    "        'answers': \"Yes\",\n",
    "        'bbox': [[54.0, 106.0, 30.0, 31.0]],\n",
    "        'image': \"xmlab1/source.jpg\",\n",
    "        'normalized_bbox': [0.21, 0.41, 0.33, 0.54],\n",
    "        'image_size': (256, 256)\n",
    "        }\n",
    "        Expected new answer:\n",
    "        {\n",
    "        \"id\": 3,\n",
    "        \"new_answer\": \"The liver is located at bounding box coordinate [0.21, 0.41, 0.33, 0.54], and it shows no abnormality.\"\n",
    "        }\n",
    "\n",
    "        Notice that you should output a list of answers that can be directly parsed by json, with the same order of the given QA list, no extra explanations or extra strings, for each question-answer pair in the batch.\n",
    "        Here are the old QA pairs that you need to process:\n",
    "        \"\"\"\n",
    "\n",
    "        full_prompt += str(batch_prompt) + \"your output of a list of answers, no extra strings:\"\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant for synthesizing new VQAs.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": full_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # print(completion.choices[0].message)\n",
    "        # Parse scores from response\n",
    "        vqa_text = completion.choices[0].message.content.strip()\n",
    "        \n",
    "        # Convert response into a list of scores, assuming each score is on a new line\n",
    "        try:\n",
    "            batch_answers = json.loads(vqa_text)\n",
    "            new_answers.extend(batch_answers)\n",
    "        except ValueError:\n",
    "            print(f\"Unexpected response format: {vqa_text}\")\n",
    "\n",
    "    return new_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sampled_gt = random.sample(prorcessed_gt, 100)\n",
    "new_answers = build_vqa_with_gpt4o_in_batches(sampled_gt, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1705,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.24, 0.33, 0.55, 0.63], and it is present in the image.'},\n",
       " {'id': 4053,\n",
       "  'new_answer': 'The spinal cord is located at bounding box coordinate [0.46, 0.56, 0.49, 0.58], and it is present in the image.'},\n",
       " {'id': 852,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.11, 0.28, 0.25, 0.63], and it is situated on the left side of the image.'},\n",
       " {'id': 961,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.12, 0.13, 0.58, 0.6], and it is present in the image.'},\n",
       " {'id': 4696,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.15, 0.37, 0.32, 0.59], and it is present in the image.'},\n",
       " {'id': 4220,\n",
       "  'new_answer': 'The colon is located at bounding box coordinate [0.4, 0.29, 0.75, 0.46], and it is dark gray in color.'},\n",
       " {'id': 3928,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.21, 0.29, 0.52, 0.61], and it is present in the image.'},\n",
       " {'id': 2080,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.24, 0.43, 0.3, 0.59], and it appears to be healthy.'},\n",
       " {'id': 2920,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.19, 0.4, 0.28, 0.59], and it is positioned to the left.'},\n",
       " {'id': 4031,\n",
       "  'new_answer': 'The liver is located at bounding box coordinate [0.2, 0.41, 0.25, 0.49], and it is present in the image.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_answers[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 855,\n",
       " 'question': '<image>\\nWhich is bigger in this image, kidney or liver?',\n",
       " 'answers': 'Liver',\n",
       " 'bbox': [[57.0, 142.0, 71.0, 182.0]],\n",
       " 'image': 'xmlab214/source.jpg',\n",
       " 'normalized_bbox': [0.11, 0.28, 0.25, 0.63],\n",
       " 'image_size': (512, 512)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prorcessed_gt[:10]\n",
    "sampled_gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sampled_gt = {item[\"id\"]: item for item in sampled_gt}\n",
    "for item in new_answers:\n",
    "    dict_sampled_gt[item[\"id\"]][\"answers\"] = item[\"new_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sampled_gt = list(dict_sampled_gt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1746,\n",
       " 'question': '<image>\\nDoes the picture contain liver?',\n",
       " 'answers': 'The liver is located at bounding box coordinate [0.14, 0.32, 0.4, 0.65], and it is present in the image.',\n",
       " 'bbox': [[71.0, 164.0, 136.0, 169.0]],\n",
       " 'image': 'xmlab290/source.jpg',\n",
       " 'normalized_bbox': [0.14, 0.32, 0.4, 0.65],\n",
       " 'image_size': (512, 512),\n",
       " 'new_answer': 'The liver is located at bounding box coordinate [0.14, 0.32, 0.4, 0.65], and it is present in the image.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sampled_gt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = []\n",
    "for i in new_sampled_gt:\n",
    "    template = dict()\n",
    "    \n",
    "    # template['answer_type'] = i['answer_type']\n",
    "    template['image'] = i['image']\n",
    "    template['id'] = i['id']\n",
    "    template['conversations'] = []\n",
    "    # template['text'] = i['question']\n",
    "\n",
    "    new_qa = {\"from\": \"human\", \"value\": i['question']}\n",
    "    new_qa2 = {\"from\": \"gpt\", \"value\": str(i['answers'])}\n",
    "    template['conversations'] += [new_qa, new_qa2]\n",
    "    # template['bboxes'] = []\n",
    "    # template['bboxes_dict'] = dict()\n",
    "    # for box_dict in image_name_segments_dict[i['img_name']]:\n",
    "    #     k, bbox = list(box_dict.items())[0]\n",
    "    #     if k.lower() in i['question'].lower():\n",
    "    #         template['bboxes'].append(bbox)\n",
    "    #         template['bboxes_dict'][k.lower()] = bbox\n",
    "    new_train_data.append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/aofei/hallucination/Slake/data/probing/sampled_gt.json\", 'w') as f:\n",
    "    json.dump(new_train_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
