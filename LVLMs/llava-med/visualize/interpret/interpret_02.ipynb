{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from visualizer import get_local\n",
    "get_local.activate()\n",
    "\n",
    "import os\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{ROOT_PATH}/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_HOME\"] = f\"{ROOT_PATH}/huggingface_cache/transformers\"\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/avc6555/research/MedH/Mitigation/LVLMs/llava-med\")\n",
    "\n",
    "from llava import LlavaLlamaForCausalLM\n",
    "from llava.conversation import conv_templates\n",
    "from llava.utils import disable_torch_init\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "\n",
    "from visualize.utils import show_image_relevance, compute_ca_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6311c0d9b91740609c16fd6f0c2f6465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 token length\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "model_name = f\"{ROOT_PATH}/LLM/llava_med\"\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "\n",
    "model_name = os.path.expanduser(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, use_cache=True).cuda()\n",
    "# print(\"vision tower\", model.config.mm_vision_tower)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=torch.float16)\n",
    "vision_tower = model.model.vision_tower[0]\n",
    "vision_tower.to(device='cuda', dtype=torch.float16)\n",
    "\n",
    "mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "if mm_use_im_start_end:\n",
    "    tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "\n",
    "vision_config = vision_tower.config\n",
    "vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "vision_config.use_im_start_end = mm_use_im_start_end\n",
    "if mm_use_im_start_end:\n",
    "    vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n",
    "image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n",
    "print(image_token_len, \"token length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "get_local.clear()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "H, W = 16, 16\n",
    "_, preprocess = clip.load(\"ViT-B/32\", device='cpu', jit=False)\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords, tokenizer, input_ids):\n",
    "        self.keywords = keywords\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_len = None\n",
    "        self.input_ids = input_ids\n",
    "        \n",
    "\n",
    "    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if self.start_len is None:\n",
    "            self.start_len = self.input_ids.shape[1]\n",
    "        else:\n",
    "            outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)[0]\n",
    "            for keyword in self.keywords:\n",
    "                if keyword in outputs:\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from /data/aofei/hallucination/mitigation/Slake/llava_med/gt_exp/only_gt/all_expert_4_8_rank16/lora_bbox_0.2/epoch9_seed4/checkpoints\n",
      "Merging weights\n",
      "load MoE parameters!\n",
      "Subset loaded successfully.\n",
      "Convert to FP16...\n"
     ]
    }
   ],
   "source": [
    "#LOAD LORA\n",
    "from llava.model.moe_llava import LoRA_MOE_FFN, LoRA_MOE_QK, LoRA_MOE_QK_old\n",
    "\n",
    "query_expert_num, key_expert_num, rank, alpha = 4, 8, 16, 8\n",
    "# query_expert_num, key_expert_num, rank, alpha = 8, 16, 16, 8\n",
    "dense_q_moe = True\n",
    "\n",
    "peft_path = \"/data/aofei/hallucination/mitigation/Slake/llava_med/gt_exp/only_gt/all_expert_4_8_rank16/lora_bbox_0.2/epoch9_seed4/checkpoints\"\n",
    "\n",
    "if peft_path is not None and len(str(peft_path))>4:\n",
    "    from peft import PeftModel\n",
    "    print(f\"Loading LoRA weights from {peft_path}\")\n",
    "    model = PeftModel.from_pretrained(model, peft_path)\n",
    "    print(f\"Merging weights\")\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    moe_path = os.path.join(peft_path, \"non_lora_trainables.bin\")\n",
    "    moe_state_dict = torch.load(moe_path, map_location='cuda')\n",
    "    if len(moe_state_dict.keys()) > 32:\n",
    "        print(\"load MoE parameters!\")\n",
    "        num_layers = len(model.base_model.layers)\n",
    "        for i in range(num_layers):\n",
    "            original_q = model.base_model.layers[i].self_attn.q_proj\n",
    "            model.base_model.layers[i].self_attn.q_proj = \\\n",
    "                LoRA_MOE_QK(args=None,\n",
    "                    lora_rank=rank,\n",
    "                    lora_alpha=alpha,\n",
    "                    num_experts=query_expert_num,\n",
    "                    original_module=original_q,\n",
    "                    dense_moe=dense_q_moe)\n",
    "        for i in range(num_layers):\n",
    "            original_k = model.base_model.layers[i].self_attn.k_proj\n",
    "            model.base_model.layers[i].self_attn.k_proj = \\\n",
    "                LoRA_MOE_QK_old(args=None,\n",
    "                    lora_rank=rank,\n",
    "                    lora_alpha=alpha,\n",
    "                    num_experts=key_expert_num,\n",
    "                    top_moe_experts=2,\n",
    "                    original_module=original_k)\n",
    "        new_state_dict = {}\n",
    "        for key, value in moe_state_dict.items():\n",
    "            # Replace \"base_model.model\" with an empty string to remove it\n",
    "            new_key = key.replace(\"base_model.model\", \"\")\n",
    "            if new_key.startswith(\".\"):\n",
    "                new_key = new_key[1:]\n",
    "            new_state_dict[new_key] = value.to(\"cuda\")\n",
    "            # new_state_dict[new_key] = value\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        model = model.to(\"cuda\")\n",
    "        for key in new_state_dict.keys():\n",
    "            assert torch.equal(model.state_dict()[key], new_state_dict[key]), f\"Mismatch in {key}\"\n",
    "        print(\"Subset loaded successfully.\")\n",
    "        print('Convert to FP16...')\n",
    "        model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_interpret(model, input_ids, images, mask, threshold=0.2):\n",
    "    # Extract the image token index\n",
    "    img_token_idx = int(torch.where(input_ids.squeeze(0) == 32001)[0][0])\n",
    "\n",
    "    # Generate model outputs with attentions\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        images=images,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=20,\n",
    "        output_attentions=True,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    # Retrieve cached attention maps\n",
    "    cache = get_local.cache\n",
    "    ori_attention_maps = [i for i in cache['LlamaSdpaAttention.forward'] if i is not None]\n",
    "    attention_maps = [att for i, att in enumerate(ori_attention_maps) if att.shape[-2] > 1]\n",
    "\n",
    "    # Compute mean attention map\n",
    "    mean_att = torch.cat(attention_maps, 0).mean(0)\n",
    "    target2img_rel = mean_att[:, img_token_idx+mask.shape[0]*mask.shape[1]:, img_token_idx:img_token_idx+mask.shape[0]*mask.shape[1]].mean(axis=0).mean(axis=0).unsqueeze(0)\n",
    "\n",
    "    # Reshape the attention map to match the image dimensions\n",
    "    avg_attention_map = target2img_rel.reshape(mask.shape)\n",
    "\n",
    "    # Normalize the attention map to [0, 1]\n",
    "    avg_attention_map = (avg_attention_map - avg_attention_map.min()) / (avg_attention_map.max() - avg_attention_map.min())\n",
    "\n",
    "    # Define a function to compute Coverage Score\n",
    "    def compute_coverage(mask, attention_map, threshold=threshold):\n",
    "        thresholded_attention = (attention_map >= threshold).float()\n",
    "        coverage = (mask * thresholded_attention).sum() / mask.sum()\n",
    "        return coverage.item()\n",
    "\n",
    "    # Define a function to compute Intensity Alignment\n",
    "    def compute_intensity_alignment(mask, attention_map):\n",
    "        intensity_alignment = (mask * attention_map).sum() / mask.sum()\n",
    "        return intensity_alignment.item()\n",
    "\n",
    "    # Compute the two metrics\n",
    "    coverage_score = compute_coverage(mask, avg_attention_map)\n",
    "    intensity_alignment = compute_intensity_alignment(mask, avg_attention_map)\n",
    "\n",
    "    # Clear cache and return output\n",
    "    get_local.clear()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"output\": tokenizer.decode(outputs[\"sequences\"][0]).strip(),\n",
    "        \"coverage_score\": coverage_score,\n",
    "        \"intensity_alignment\": intensity_alignment,\n",
    "        \"mask\":mask.detach().cpu(),\n",
    "        \"attention_map\": avg_attention_map.detach().cpu()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image': 'xmlab102/source.jpg',\n",
       "  'id': 11941,\n",
       "  'organ': 'Lung',\n",
       "  'answer_type': 'OPEN',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nWhat diseases are included in the picture?'},\n",
       "   {'from': 'gpt', 'value': 'Lung Cancer'}],\n",
       "  'bboxes': [[144, 150, 9, 11]],\n",
       "  'gt_bboxes': [[144, 150, 9, 11]]},\n",
       " 206)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "test_gt_path = \"/data/aofei/hallucination/Slake/data/with_gt/test_with_gt.json\"\n",
    "with open(test_gt_path) as f:\n",
    "    test_gt = json.load(f)\n",
    "test_gt[0], len(test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_local.clear()\n",
    "torch.cuda.empty_cache()\n",
    "inference_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "n_px = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(n_px, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.CenterCrop(n_px),\n",
    "    transforms.Resize(H, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [04:52<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ih, iw = 256, 256\n",
    "for idx, item in tqdm(list(enumerate(test_gt[5:]))):\n",
    "    get_local.clear()\n",
    "    torch.cuda.empty_cache()\n",
    "    _id = item['id']\n",
    "    question = item['conversations'][0]['value']\n",
    "    image_path = os.path.join(\"/data/aofei/hallucination/Slake/imgs\",item['image']) \n",
    "    gt_answer = item['conversations'][1]['value']\n",
    "    inference_item = {\"question_id\":_id, 'question': question, 'gt': gt_answer}\n",
    "    gt_bboxes = item['gt_bboxes']\n",
    "\n",
    "    qs = question.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "    images = image_tensor.unsqueeze(0).half().cuda()\n",
    "    if getattr(model.config, 'mm_use_im_start_end', False):\n",
    "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN + '\\n' +  qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + '\\n' + qs\n",
    "    cur_prompt = '<image>' + '\\n' +  qs\n",
    "    conv_mode = \"simple\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.messages = []\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    prompt = conv.get_prompt()\n",
    "    inputs = tokenizer([prompt])\n",
    "    input_ids = torch.as_tensor(inputs.input_ids).cuda()\n",
    "    mask = torch.zeros(size=(ih, iw))\n",
    "    for bbox in gt_bboxes:\n",
    "        x_min, y_min, x_max, y_max = int(bbox[0]), int(bbox[1]), int(bbox[0]) + int(bbox[2]), int(bbox[1]) + int(bbox[3])\n",
    "        mask[y_min: y_max, x_min: x_max] = 1\n",
    "    mask = transform(mask.numpy())[0]\n",
    "    mask = mask.cuda() if torch.cuda.is_available() else mask\n",
    "    output = measure_interpret(model, input_ids, images, mask, threshold=0.05)\n",
    "    \n",
    "    inference_results.append(output)\n",
    "    get_local.clear()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22666302749222547, 0.11998396067313813)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference_results\n",
    "def compute_coverage(mask, attention_map, threshold=0.15):\n",
    "    thresholded_attention = (attention_map >= threshold).float()\n",
    "    coverage = (mask * thresholded_attention).sum() / mask.sum()\n",
    "    return coverage.item()\n",
    "\n",
    "# Define a function to compute Intensity Alignment\n",
    "def compute_intensity_alignment(mask, attention_map):\n",
    "    intensity_alignment = (mask * attention_map).sum() / mask.sum()\n",
    "    return intensity_alignment.item()\n",
    "\n",
    "new_coverage_scores = []\n",
    "new_intensity_alignment = []\n",
    "for i in inference_results:\n",
    "    mask = i['mask']\n",
    "    attention_map = i['attention_map']\n",
    "    coverage_score = compute_coverage(mask, attention_map)\n",
    "    intensity_alignment = compute_intensity_alignment(mask, attention_map)\n",
    "    new_coverage_scores.append(coverage_score)\n",
    "    new_intensity_alignment.append(intensity_alignment)\n",
    "\n",
    "np.nanmean(new_coverage_scores), np.nanmean(new_intensity_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5922559349980187, 0.12084772437810898)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference_results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "average_coverage = np.nanmean([i['coverage_score'] for i in inference_results])\n",
    "average_intensity = np.nanmean([i['intensity_alignment'] for i in inference_results])\n",
    "average_coverage, average_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inference_results:\n",
    "    i['text'] = i['output']\n",
    "\n",
    "with open(\"gt_results/ori_inference.jsonl\", \"w\") as f:\n",
    "    for item in inference_results:\n",
    "        f.write(json.dumps(item)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the root path\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "# Construct the command\n",
    "command = [\n",
    "    \"python\", \"/home/avc6555/research/MedH/Mitigation/LVLMs/llava-med/llava/eval/run_eval.py\",\n",
    "    \"--gt\", f\"{ROOT_PATH}/hallucination/{dataset}/data/with_gt/test_with_gt.json\",\n",
    "    \"--pred\", f\"/home/avc6555/research/MedH/visualize/llava_visualize/case_llava_med/gt_results/ori_inference.jsonl\",\n",
    "    \"--eval_res\", f\"/home/avc6555/research/MedH/visualize/llava_visualize/case_llava_med/gt_results/eval_res_ori.txt\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print the output\n",
    "# print(\"Output:\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
