{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd    \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from eval_metrics.evaluate_metrics import calculate_exactmatch, calculate_f1score, bleu, calculate_appearance_with_normalization\n",
    "from tabulate import tabulate\n",
    "from eval_metrics.glossary import *\n",
    "\n",
    "def evaluate(gt, pred, return_pred=False):\n",
    "    bleu_scores = collections.defaultdict(list)\n",
    "    exact_scores = collections.defaultdict(list)\n",
    "    f1_scores = collections.defaultdict(list)\n",
    "    question_analysis = []  # To store detailed scores for each question\n",
    "    \n",
    "    num_open = 0\n",
    "    for gt_item, pred_item in zip(gt, pred):\n",
    "        try:\n",
    "            gt_results = gt_item['conversations']\n",
    "        except KeyError:\n",
    "            gt_results = gt_item['conversatons']\n",
    "        if not pred_item.__contains__('gt'):\n",
    "            pred_item['gt'] = gt_results[1]['value']\n",
    "\n",
    "        gt_value = gt_results[1]['value'].lower()\n",
    "        pred_value = pred_item['text'].lower()\n",
    "        if pred_value.startswith('assistant:'):\n",
    "            pred_value = pred_value[10:].strip()\n",
    "\n",
    "        gt_value = normalize_word(gt_value)\n",
    "        pred_value = normalize_word(pred_value)\n",
    "\n",
    "        if gt_item['answer_type'] == 'OPEN':\n",
    "            num_open += 1\n",
    "\n",
    "            question_id = pred_item['question_id']\n",
    "            exact_match = calculate_exactmatch(pred_value, gt_value)\n",
    "            f1, precision, recall = calculate_f1score(pred_value, gt_value)\n",
    "            bleu = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split())\n",
    "            bleu_1 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(1, 0, 0, 0))\n",
    "            bleu_2 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 1, 0, 0))\n",
    "            bleu_3 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 0, 1, 0))\n",
    "\n",
    "            # Store detailed scores for each question\n",
    "            question_analysis.append({\n",
    "                'question_id': question_id,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'bleu': bleu,\n",
    "                'bleu_1': bleu_1,\n",
    "                'bleu_2': bleu_2,\n",
    "                'bleu_3': bleu_3\n",
    "            })\n",
    "\n",
    "            exact_scores['hit'].append(exact_match)\n",
    "            f1_scores['f1'].append(f1)\n",
    "            f1_scores['precision'].append(precision)\n",
    "            f1_scores['recall'].append(recall)\n",
    "            bleu_scores['bleu_score'].append(bleu)\n",
    "            bleu_scores['bleu_score_1'].append(bleu_1)\n",
    "            bleu_scores['bleu_score_2'].append(bleu_2)\n",
    "            bleu_scores['bleu_score_3'].append(bleu_3)\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    exact_score = sum(exact_scores['hit']) / len(exact_scores['hit']) if num_open else 0\n",
    "    f1_score = sum(f1_scores['f1']) / len(f1_scores['f1']) if num_open else 0\n",
    "    precision = sum(f1_scores['precision']) / len(f1_scores['precision']) if num_open else 0\n",
    "    recall = sum(f1_scores['recall']) / len(f1_scores['recall']) if num_open else 0\n",
    "    bleu_score = sum(bleu_scores['bleu_score']) / len(bleu_scores['bleu_score']) if num_open else 0\n",
    "    bleu_score_1 = sum(bleu_scores['bleu_score_1']) / len(bleu_scores['bleu_score_1']) if num_open else 0\n",
    "    bleu_score_2 = sum(bleu_scores['bleu_score_2']) / len(bleu_scores['bleu_score_2']) if num_open else 0\n",
    "    bleu_score_3 = sum(bleu_scores['bleu_score_3']) / len(bleu_scores['bleu_score_3']) if num_open else 0\n",
    "\n",
    "    # Print summary metrics\n",
    "    print(f'num_open {num_open}')\n",
    "    print(tabulate(\n",
    "        [\n",
    "            ['exact match score', exact_score * 100], \n",
    "            ['f1 score', f1_score * 100], \n",
    "            ['precision', precision * 100], \n",
    "            ['recall', recall * 100], \n",
    "            ['bleu_score', bleu_score * 100], \n",
    "            ['bleu_score_1', bleu_score_1 * 100], \n",
    "            ['bleu_score_2', bleu_score_2 * 100], \n",
    "            ['bleu_score_3', bleu_score_3 * 100]\n",
    "        ], \n",
    "        headers=['Metric', 'Performance']\n",
    "    ))\n",
    "\n",
    "    # Sort question analysis by lowest F1 score and return the top 30\n",
    "    low_performance_questions = sorted(question_analysis, key=lambda x: x['f1'])\n",
    "    if return_pred:\n",
    "        return low_performance_questions, pred\n",
    "    return low_performance_questions\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.LoRA tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 419 || num_pred_ids: 419\n",
      "num_open 300\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score      76.7574\n",
      "f1 score               76.6147\n",
      "precision              76.9241\n",
      "recall                 76.9606\n",
      "bleu_score              0.333333\n",
      "bleu_score_1           75.8185\n",
      "bleu_score_2           14.7748\n",
      "bleu_score_3            3.44444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "dir=f\"llava_med/moe_img_dense_all_query/all_expert_8_16_rank16/lora_{visual_enhance_ratio}_bbox_{bbox_ratio}/epoch{epoch_num}\"\n",
    "# gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/test.json\"\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_lung.json\"\n",
    "# pred_file = f\"{ROOT_PATH}/hallucination/mitigation/{dataset}/{dir}/inference/pred.jsonl\"\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_lung/lora/epoch9_seed4/inference/pred.jsonl\"\n",
    "# bv_pred_path = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_lung/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results, pred = evaluate(gt, pred, return_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 11984,\n",
       " 'exact_match': 1.0,\n",
       " 'f1': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'bleu': 1.821831989445342e-231,\n",
       " 'bleu_1': 1.0,\n",
       " 'bleu_2': 2.2250738585072626e-308,\n",
       " 'bleu_3': 2.2250738585072626e-308}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict()\n",
    "for i in pred:\n",
    "    pred_dict[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top 100 low performance questions\n",
    "top_low_performance = [item for item in results if item['f1'] < 0.1]\n",
    "top_low_performance_ids = [item['question_id'] for item in top_low_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12968,\n",
       "  'exact_match': 0.0,\n",
       "  'f1': 0,\n",
       "  'precision': 0,\n",
       "  'recall': 0,\n",
       "  'bleu': 0,\n",
       "  'bleu_1': 0,\n",
       "  'bleu_2': 0,\n",
       "  'bleu_3': 0},\n",
       " 52)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_low_performance[-1], len(top_low_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 419 || num_pred_ids: 419\n",
      "num_open 300\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score      82.0046\n",
      "f1 score               81.8196\n",
      "precision              82.213\n",
      "recall                 82.2096\n",
      "bleu_score              0.333333\n",
      "bleu_score_1           80.744\n",
      "bleu_score_2           16.9418\n",
      "bleu_score_3            4.51138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_lung.json\"\n",
    "\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_lung/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results_ours, pred_ours = evaluate(gt, pred, return_pred=True)\n",
    "\n",
    "pred_dict_ours = dict()\n",
    "for i in pred_ours:\n",
    "    pred_dict_ours[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top high performance questions\n",
    "top_high_performance = [item for item in results_ours if item['f1'] > 0.5]\n",
    "top_high_performance_ids = [item['question_id'] for item in top_high_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12988,\n",
       "  'exact_match': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'bleu': 1.821831989445342e-231,\n",
       "  'bleu_1': 1.0,\n",
       "  'bleu_2': 2.2250738585072626e-308,\n",
       "  'bleu_3': 2.2250738585072626e-308},\n",
       " 242)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_high_performance[-1], len(top_high_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_better_ids = set(top_low_performance_ids) & set(top_high_performance_ids)\n",
    "len(our_better_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 12038\n",
      "GT: Lung\n",
      "LoRA: {'question_id': 12038, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Heart', 'gt': 'Lung', 'answer_id': 'eEyVA6xFagLx8dJ4T2YTeY', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lung\n",
      "\n",
      "Question ID: 12050\n",
      "GT: Pneumonia\n",
      "LoRA: {'question_id': 12050, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Cardiomegaly', 'gt': 'Pneumonia', 'answer_id': 'T23NyNEc7WFPbYn54ymRs2', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Pneumonia\n",
      "\n",
      "Question ID: 12054\n",
      "GT: Lung\n",
      "LoRA: {'question_id': 12054, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Heart', 'gt': 'Lung', 'answer_id': 'BXtm7UHfSHiyFjCEmVaYnD', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lung\n",
      "\n",
      "Question ID: 12070\n",
      "GT: Pneumothorax\n",
      "LoRA: {'question_id': 12070, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Cardiomegaly', 'gt': 'Pneumothorax', 'answer_id': 'XpkJZXXJJUhs2gXeJ3cXJ2', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Pneumothorax\n",
      "\n",
      "Question ID: 12071\n",
      "GT: Right Lung, Left\n",
      "LoRA: {'question_id': 12071, 'prompt': '<image>\\nWhere is/are the abnormality located?', 'text': 'Assistant: Center', 'gt': 'Right Lung, Left', 'answer_id': 'Qi485be37BB8PGgmMiw32Z', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Left Lung, Right\n",
      "\n",
      "Question ID: 12327\n",
      "GT: Right\n",
      "LoRA: {'question_id': 12327, 'prompt': '<image>\\nWhich side of lung is abnormal in this image,left or right?', 'text': 'Assistant: Left', 'gt': 'Right', 'answer_id': 'cwU34tViuBNxsTRmbdjTPZ', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Right\n",
      "\n",
      "Question ID: 12074\n",
      "GT: Lung\n",
      "LoRA: {'question_id': 12074, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Heart', 'gt': 'Lung', 'answer_id': 'grPdGxHQ8KhxowAdPUE6jH', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lung\n",
      "\n",
      "Question ID: 12346\n",
      "GT: Cardiomegaly\n",
      "LoRA: {'question_id': 12346, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Pulmonary Mass', 'gt': 'Cardiomegaly', 'answer_id': 'JQwXDGYGHcTpHX4rTZQJEY', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Cardiomegaly\n",
      "\n",
      "Question ID: 12477\n",
      "GT: Right\n",
      "LoRA: {'question_id': 12477, 'prompt': '<image>\\nWhich side of lung is abnormal in this image?', 'text': 'Assistant: Both', 'gt': 'Right', 'answer_id': 'Mq2jniARE9Hhkppsmf9hvm', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Right\n",
      "\n",
      "Question ID: 11985\n",
      "GT: Left Lung, Lower Right\n",
      "LoRA: {'question_id': 11985, 'prompt': '<image>\\nWhere is/are the abnormality located?', 'text': 'Assistant: Both Lungs', 'gt': 'Left Lung, Lower Right', 'answer_id': '3QhrLVJozBDd2BCkkZBfDh', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Left Lung, Right\n",
      "\n",
      "Question ID: 12372\n",
      "GT: Lower Left Lung\n",
      "LoRA: {'question_id': 12372, 'prompt': '<image>\\nWhere is the abnormality in this image?', 'text': 'Assistant: Heart', 'gt': 'Lower Left Lung', 'answer_id': 'oWh4mnXxkzFrMbewMZT46f', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lower Left Lung\n",
      "\n",
      "Question ID: 11991\n",
      "GT: Cardiomegaly\n",
      "LoRA: {'question_id': 11991, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Pleural Effusion', 'gt': 'Cardiomegaly', 'answer_id': 'coMFeFRcXNXKjHTshCNxYL', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Cardiomegaly\n",
      "\n",
      "Question ID: 11992\n",
      "GT: Center\n",
      "LoRA: {'question_id': 11992, 'prompt': '<image>\\nWhere is/are the abnormality located?', 'text': 'Assistant: Left Lung, Right', 'gt': 'Center', 'answer_id': 'A8Tpaur6Lqq2srtBZyLcTq', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Center\n",
      "\n",
      "Question ID: 12006\n",
      "GT: Right Lung\n",
      "LoRA: {'question_id': 12006, 'prompt': '<image>\\nWhere is/are the abnormality located?', 'text': 'Assistant: Center', 'gt': 'Right Lung', 'answer_id': 'cuiJN3KAb9zsQR3mW9zwdv', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Right Lung, Left\n",
      "\n",
      "Question ID: 12008\n",
      "GT: Lung\n",
      "LoRA: {'question_id': 12008, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Heart', 'gt': 'Lung', 'answer_id': 'CuXuQqGpmBCbBidpLRLXMc', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lung\n",
      "\n",
      "Question ID: 12402\n",
      "GT: Upper Left Lung\n",
      "LoRA: {'question_id': 12402, 'prompt': '<image>\\nWhere is the abnormality in this image?', 'text': 'Assistant: Heart', 'gt': 'Upper Left Lung', 'answer_id': 'SvCN9M3SSrD6cBP7xRsjg9', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lower Left Lung\n",
      "\n",
      "Question ID: 12020\n",
      "GT: Cardiomegaly\n",
      "LoRA: {'question_id': 12020, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Pneumonia', 'gt': 'Cardiomegaly', 'answer_id': 'JGwd79u8Y4roAngC8uNxhq', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Cardiomegaly\n",
      "\n",
      "Question ID: 12022\n",
      "GT: Heart\n",
      "LoRA: {'question_id': 12022, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Lung', 'gt': 'Heart', 'answer_id': 'NuMDq8dTXvkpXY3jqyyAdh', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Heart\n",
      "\n",
      "Question ID: 12023\n",
      "GT: X-Ray\n",
      "LoRA: {'question_id': 12023, 'prompt': '<image>\\nWhat modality is used to take this image?', 'text': '', 'gt': 'X-Ray', 'answer_id': 'K9UvpECfc8hnVa5jEKXAPE', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: X-Ray\n",
      "\n",
      "Question ID: 12030\n",
      "GT: Lung\n",
      "LoRA: {'question_id': 12030, 'prompt': '<image>\\nWhich organ is abnormal, heart or lung?', 'text': 'Assistant: Heart', 'gt': 'Lung', 'answer_id': 'ggcXbUbXCSVDg3Yx9rNDot', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lung\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _id in our_better_ids:\n",
    "    print(f\"Question ID: {_id}\")\n",
    "    print(f\"GT: {pred_dict[_id]['gt']}\")\n",
    "    print(f\"LoRA: {pred_dict[_id]}\")\n",
    "    print(f\"Ours (beam): {pred_dict_ours[_id]['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
