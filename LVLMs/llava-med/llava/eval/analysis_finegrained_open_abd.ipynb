{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd    \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from eval_metrics.evaluate_metrics import calculate_exactmatch, calculate_f1score, bleu, calculate_appearance_with_normalization\n",
    "from tabulate import tabulate\n",
    "from eval_metrics.glossary import *\n",
    "\n",
    "def evaluate(gt, pred, return_pred=False):\n",
    "    bleu_scores = collections.defaultdict(list)\n",
    "    exact_scores = collections.defaultdict(list)\n",
    "    f1_scores = collections.defaultdict(list)\n",
    "    question_analysis = []  # To store detailed scores for each question\n",
    "    \n",
    "    num_open = 0\n",
    "    for gt_item, pred_item in zip(gt, pred):\n",
    "        try:\n",
    "            gt_results = gt_item['conversations']\n",
    "        except KeyError:\n",
    "            gt_results = gt_item['conversatons']\n",
    "        if not pred_item.__contains__('gt'):\n",
    "            pred_item['gt'] = gt_results[1]['value']\n",
    "\n",
    "        gt_value = gt_results[1]['value'].lower()\n",
    "        pred_value = pred_item['text'].lower()\n",
    "        if pred_value.startswith('assistant:'):\n",
    "            pred_value = pred_value[10:].strip()\n",
    "\n",
    "        gt_value = normalize_word(gt_value)\n",
    "        pred_value = normalize_word(pred_value)\n",
    "\n",
    "        if gt_item['answer_type'] == 'OPEN':\n",
    "            num_open += 1\n",
    "\n",
    "            question_id = pred_item['question_id']\n",
    "            exact_match = calculate_exactmatch(pred_value, gt_value)\n",
    "            f1, precision, recall = calculate_f1score(pred_value, gt_value)\n",
    "            bleu = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split())\n",
    "            bleu_1 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(1, 0, 0, 0))\n",
    "            bleu_2 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 1, 0, 0))\n",
    "            bleu_3 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 0, 1, 0))\n",
    "\n",
    "            # Store detailed scores for each question\n",
    "            question_analysis.append({\n",
    "                'question_id': question_id,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'bleu': bleu,\n",
    "                'bleu_1': bleu_1,\n",
    "                'bleu_2': bleu_2,\n",
    "                'bleu_3': bleu_3\n",
    "            })\n",
    "\n",
    "            exact_scores['hit'].append(exact_match)\n",
    "            f1_scores['f1'].append(f1)\n",
    "            f1_scores['precision'].append(precision)\n",
    "            f1_scores['recall'].append(recall)\n",
    "            bleu_scores['bleu_score'].append(bleu)\n",
    "            bleu_scores['bleu_score_1'].append(bleu_1)\n",
    "            bleu_scores['bleu_score_2'].append(bleu_2)\n",
    "            bleu_scores['bleu_score_3'].append(bleu_3)\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    exact_score = sum(exact_scores['hit']) / len(exact_scores['hit']) if num_open else 0\n",
    "    f1_score = sum(f1_scores['f1']) / len(f1_scores['f1']) if num_open else 0\n",
    "    precision = sum(f1_scores['precision']) / len(f1_scores['precision']) if num_open else 0\n",
    "    recall = sum(f1_scores['recall']) / len(f1_scores['recall']) if num_open else 0\n",
    "    bleu_score = sum(bleu_scores['bleu_score']) / len(bleu_scores['bleu_score']) if num_open else 0\n",
    "    bleu_score_1 = sum(bleu_scores['bleu_score_1']) / len(bleu_scores['bleu_score_1']) if num_open else 0\n",
    "    bleu_score_2 = sum(bleu_scores['bleu_score_2']) / len(bleu_scores['bleu_score_2']) if num_open else 0\n",
    "    bleu_score_3 = sum(bleu_scores['bleu_score_3']) / len(bleu_scores['bleu_score_3']) if num_open else 0\n",
    "\n",
    "    # Print summary metrics\n",
    "    print(f'num_open {num_open}')\n",
    "    print(tabulate(\n",
    "        [\n",
    "            ['exact match score', exact_score * 100], \n",
    "            ['f1 score', f1_score * 100], \n",
    "            ['precision', precision * 100], \n",
    "            ['recall', recall * 100], \n",
    "            ['bleu_score', bleu_score * 100], \n",
    "            ['bleu_score_1', bleu_score_1 * 100], \n",
    "            ['bleu_score_2', bleu_score_2 * 100], \n",
    "            ['bleu_score_3', bleu_score_3 * 100]\n",
    "        ], \n",
    "        headers=['Metric', 'Performance']\n",
    "    ))\n",
    "\n",
    "    # Sort question analysis by lowest F1 score and return the top 30\n",
    "    low_performance_questions = sorted(question_analysis, key=lambda x: x['f1'])\n",
    "    if return_pred:\n",
    "        return low_performance_questions, pred\n",
    "    return low_performance_questions\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.LoRA tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 313 || num_pred_ids: 313\n",
      "num_open 191\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score       79.6684\n",
      "f1 score                79.1449\n",
      "precision               79.6684\n",
      "recall                  79.6684\n",
      "bleu_score               3.66492\n",
      "bleu_score_1            77.9793\n",
      "bleu_score_2            19.8953\n",
      "bleu_score_3             3.66492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "dir=f\"llava_med/moe_img_dense_all_query/all_expert_8_16_rank16/lora_{visual_enhance_ratio}_bbox_{bbox_ratio}/epoch{epoch_num}\"\n",
    "# gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/test.json\"\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_abd.json\"\n",
    "# pred_file = f\"{ROOT_PATH}/hallucination/mitigation/{dataset}/{dir}/inference/pred.jsonl\"\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_abd/lora/epoch9_seed4/inference/pred.jsonl\"\n",
    "# bv_pred_path = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_lung/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results, pred = evaluate(gt, pred, return_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 12171,\n",
       " 'exact_match': 1.0,\n",
       " 'f1': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'bleu': 1.821831989445342e-231,\n",
       " 'bleu_1': 1.0,\n",
       " 'bleu_2': 2.2250738585072626e-308,\n",
       " 'bleu_3': 2.2250738585072626e-308}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict()\n",
    "for i in pred:\n",
    "    pred_dict[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top 100 low performance questions\n",
    "top_low_performance = [item for item in results if item['f1'] < 0.1]\n",
    "top_low_performance_ids = [item['question_id'] for item in top_low_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12991,\n",
       "  'exact_match': 0.0,\n",
       "  'f1': 0,\n",
       "  'precision': 0,\n",
       "  'recall': 0,\n",
       "  'bleu': 0,\n",
       "  'bleu_1': 0,\n",
       "  'bleu_2': 0,\n",
       "  'bleu_3': 0},\n",
       " 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_low_performance[-1], len(top_low_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 313 || num_pred_ids: 313\n",
      "num_open 191\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score       82.3735\n",
      "f1 score                81.6928\n",
      "precision               82.3735\n",
      "recall                  81.7627\n",
      "bleu_score               3.66492\n",
      "bleu_score_1            80.8094\n",
      "bleu_score_2            20.4188\n",
      "bleu_score_3             3.66492\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_abd.json\"\n",
    "\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_abd/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results_ours, pred_ours = evaluate(gt, pred, return_pred=True)\n",
    "\n",
    "pred_dict_ours = dict()\n",
    "for i in pred_ours:\n",
    "    pred_dict_ours[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top high performance questions\n",
    "top_high_performance = [item for item in results_ours if item['f1'] > 0.5]\n",
    "top_high_performance_ids = [item['question_id'] for item in top_high_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12989,\n",
       "  'exact_match': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'bleu': 1.821831989445342e-231,\n",
       "  'bleu_1': 1.0,\n",
       "  'bleu_2': 2.2250738585072626e-308,\n",
       "  'bleu_3': 2.2250738585072626e-308},\n",
       " 155)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_high_performance[-1], len(top_high_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_better_ids = set(top_low_performance_ids) & set(top_high_performance_ids)\n",
    "len(our_better_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 12100\n",
      "GT: Top\n",
      "LoRA: {'question_id': 12100, 'prompt': '<image>\\nWhere is the liver?', 'text': 'Assistant: Upper Left', 'gt': 'Top', 'answer_id': 'awoWEAVnNAsyH7J3rM3MFP', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Top\n",
      "\n",
      "Question ID: 12807\n",
      "GT: Colon\n",
      "LoRA: {'question_id': 12807, 'prompt': '<image>\\nWhich is smaller in this image,colon or liver?', 'text': 'Assistant: Liver', 'gt': 'Colon', 'answer_id': 'FdwMXJHHY6KucVFTgyiu23', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Colon\n",
      "\n",
      "Question ID: 12201\n",
      "GT: 2\n",
      "LoRA: {'question_id': 12201, 'prompt': '<image>\\nHow many kidneys in this image?', 'text': 'Assistant: 1', 'gt': '2', 'answer_id': 'K8yQEc5aBQSDFmdbKRxvo5', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: 2\n",
      "\n",
      "Question ID: 12882\n",
      "GT: Esophagus\n",
      "LoRA: {'question_id': 12882, 'prompt': '<image>\\nWhich is smaller in this image, esophagus or liver?', 'text': 'Assistant: Liver', 'gt': 'Esophagus', 'answer_id': 'hu4A5PcyDd54vFmvnXj67C', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Esophagus\n",
      "\n",
      "Question ID: 12180\n",
      "GT: Upper Left\n",
      "LoRA: {'question_id': 12180, 'prompt': '<image>\\nWhere is the liver?', 'text': 'Assistant: Top', 'gt': 'Upper Left', 'answer_id': '6TRniNMAjWSJVPAizjHUwt', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Upper Left\n",
      "\n",
      "Question ID: 12118\n",
      "GT: Spleen\n",
      "LoRA: {'question_id': 12118, 'prompt': '<image>\\nWhat is the organ on the lower right corner of the body in this image?', 'text': 'Assistant: Left Lung', 'gt': 'Spleen', 'answer_id': 'XgPjsTERkFyCqZWXK6kBBd', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Spleen\n",
      "\n",
      "Question ID: 12087\n",
      "GT: Left Kidney\n",
      "LoRA: {'question_id': 12087, 'prompt': '<image>\\nWhat is the organ located in the lower right corner of the body in this image?', 'text': 'Assistant: Spleen', 'gt': 'Left Kidney', 'answer_id': 'WmFRFm2j58urXyhmoJAQZ3', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Left Kidney\n",
      "\n",
      "Question ID: 12093\n",
      "GT: 4\n",
      "LoRA: {'question_id': 12093, 'prompt': '<image>\\nHow many organs are there in this image?', 'text': 'Assistant: 5', 'gt': '4', 'answer_id': 'hMVaHoMo7DH2WRvzCDgpzX', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: 4\n",
      "\n",
      "Question ID: 12799\n",
      "GT: 2\n",
      "LoRA: {'question_id': 12799, 'prompt': '<image>\\nHow many lungs have existed in this image?', 'text': 'Assistant: 0', 'gt': '2', 'answer_id': 'WjxfsHRB3hb4tRMhyVPSmF', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _id in our_better_ids:\n",
    "    print(f\"Question ID: {_id}\")\n",
    "    print(f\"GT: {pred_dict[_id]['gt']}\")\n",
    "    print(f\"LoRA: {pred_dict[_id]}\")\n",
    "    print(f\"Ours (beam): {pred_dict_ours[_id]['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
