{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd    \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from eval_metrics.evaluate_metrics import calculate_exactmatch, calculate_f1score, bleu, calculate_appearance_with_normalization\n",
    "from tabulate import tabulate\n",
    "from eval_metrics.glossary import *\n",
    "\n",
    "def evaluate(gt, pred, return_pred=False):\n",
    "    bleu_scores = collections.defaultdict(list)\n",
    "    exact_scores = collections.defaultdict(list)\n",
    "    f1_scores = collections.defaultdict(list)\n",
    "    question_analysis = []  # To store detailed scores for each question\n",
    "    \n",
    "    num_open = 0\n",
    "    for gt_item, pred_item in zip(gt, pred):\n",
    "        try:\n",
    "            gt_results = gt_item['conversations']\n",
    "        except KeyError:\n",
    "            gt_results = gt_item['conversatons']\n",
    "        if not pred_item.__contains__('gt'):\n",
    "            pred_item['gt'] = gt_results[1]['value']\n",
    "\n",
    "        gt_value = gt_results[1]['value'].lower()\n",
    "        pred_value = pred_item['text'].lower()\n",
    "        if pred_value.startswith('assistant:'):\n",
    "            pred_value = pred_value[10:].strip()\n",
    "\n",
    "        gt_value = normalize_word(gt_value)\n",
    "        pred_value = normalize_word(pred_value)\n",
    "\n",
    "        if gt_item['answer_type'] == 'OPEN':\n",
    "            num_open += 1\n",
    "\n",
    "            question_id = pred_item['question_id']\n",
    "            exact_match = calculate_exactmatch(pred_value, gt_value)\n",
    "            f1, precision, recall = calculate_f1score(pred_value, gt_value)\n",
    "            bleu = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split())\n",
    "            bleu_1 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(1, 0, 0, 0))\n",
    "            bleu_2 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 1, 0, 0))\n",
    "            bleu_3 = sentence_bleu(references=[str(gt_value).split()], hypothesis=str(pred_value).split(), weights=(0, 0, 1, 0))\n",
    "\n",
    "            # Store detailed scores for each question\n",
    "            question_analysis.append({\n",
    "                'question_id': question_id,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'bleu': bleu,\n",
    "                'bleu_1': bleu_1,\n",
    "                'bleu_2': bleu_2,\n",
    "                'bleu_3': bleu_3\n",
    "            })\n",
    "\n",
    "            exact_scores['hit'].append(exact_match)\n",
    "            f1_scores['f1'].append(f1)\n",
    "            f1_scores['precision'].append(precision)\n",
    "            f1_scores['recall'].append(recall)\n",
    "            bleu_scores['bleu_score'].append(bleu)\n",
    "            bleu_scores['bleu_score_1'].append(bleu_1)\n",
    "            bleu_scores['bleu_score_2'].append(bleu_2)\n",
    "            bleu_scores['bleu_score_3'].append(bleu_3)\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    exact_score = sum(exact_scores['hit']) / len(exact_scores['hit']) if num_open else 0\n",
    "    f1_score = sum(f1_scores['f1']) / len(f1_scores['f1']) if num_open else 0\n",
    "    precision = sum(f1_scores['precision']) / len(f1_scores['precision']) if num_open else 0\n",
    "    recall = sum(f1_scores['recall']) / len(f1_scores['recall']) if num_open else 0\n",
    "    bleu_score = sum(bleu_scores['bleu_score']) / len(bleu_scores['bleu_score']) if num_open else 0\n",
    "    bleu_score_1 = sum(bleu_scores['bleu_score_1']) / len(bleu_scores['bleu_score_1']) if num_open else 0\n",
    "    bleu_score_2 = sum(bleu_scores['bleu_score_2']) / len(bleu_scores['bleu_score_2']) if num_open else 0\n",
    "    bleu_score_3 = sum(bleu_scores['bleu_score_3']) / len(bleu_scores['bleu_score_3']) if num_open else 0\n",
    "\n",
    "    # Print summary metrics\n",
    "    print(f'num_open {num_open}')\n",
    "    print(tabulate(\n",
    "        [\n",
    "            ['exact match score', exact_score * 100], \n",
    "            ['f1 score', f1_score * 100], \n",
    "            ['precision', precision * 100], \n",
    "            ['recall', recall * 100], \n",
    "            ['bleu_score', bleu_score * 100], \n",
    "            ['bleu_score_1', bleu_score_1 * 100], \n",
    "            ['bleu_score_2', bleu_score_2 * 100], \n",
    "            ['bleu_score_3', bleu_score_3 * 100]\n",
    "        ], \n",
    "        headers=['Metric', 'Performance']\n",
    "    ))\n",
    "\n",
    "    # Sort question analysis by lowest F1 score and return the top 30\n",
    "    low_performance_questions = sorted(question_analysis, key=lambda x: x['f1'])\n",
    "    if return_pred:\n",
    "        return low_performance_questions, pred\n",
    "    return low_performance_questions\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.LoRA tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 208 || num_pred_ids: 208\n",
      "num_open 145\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score       76.8766\n",
      "f1 score                78.8731\n",
      "precision               79.116\n",
      "recall                  79.1461\n",
      "bleu_score               8.45489\n",
      "bleu_score_1            76.6281\n",
      "bleu_score_2            16.478\n",
      "bleu_score_3             8.95514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "dir=f\"llava_med/moe_img_dense_all_query/all_expert_8_16_rank16/lora_{visual_enhance_ratio}_bbox_{bbox_ratio}/epoch{epoch_num}\"\n",
    "# gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/test.json\"\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_brain.json\"\n",
    "# pred_file = f\"{ROOT_PATH}/hallucination/mitigation/{dataset}/{dir}/inference/pred.jsonl\"\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_brain/lora/epoch9_seed4/inference/pred.jsonl\"\n",
    "# bv_pred_path = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_lung/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results, pred = evaluate(gt, pred, return_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 12723,\n",
       " 'exact_match': 1.0,\n",
       " 'f1': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'bleu': 1.821831989445342e-231,\n",
       " 'bleu_1': 1.0,\n",
       " 'bleu_2': 2.2250738585072626e-308,\n",
       " 'bleu_3': 2.2250738585072626e-308}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict()\n",
    "for i in pred:\n",
    "    pred_dict[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top 100 low performance questions\n",
    "top_low_performance = [item for item in results if item['f1'] < 0.1]\n",
    "top_low_performance_ids = [item['question_id'] for item in top_low_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12779,\n",
       "  'exact_match': 0.0,\n",
       "  'f1': 0,\n",
       "  'precision': 0,\n",
       "  'recall': 0,\n",
       "  'bleu': 0,\n",
       "  'bleu_1': 0,\n",
       "  'bleu_2': 0,\n",
       "  'bleu_3': 0},\n",
       " 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_low_performance[-1], len(top_low_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " organ\n",
      "num_gt_ids: 208 || num_pred_ids: 208\n",
      "num_open 145\n",
      "Metric               Performance\n",
      "-----------------  -------------\n",
      "exact match score       81.1864\n",
      "f1 score                81.9051\n",
      "precision               82.6327\n",
      "recall                  81.7044\n",
      "bleu_score               7.78659\n",
      "bleu_score_1            79.7784\n",
      "bleu_score_2            18.406\n",
      "bleu_score_3             7.93934\n"
     ]
    }
   ],
   "source": [
    "# def eval_open_file(gt_file, pred_file):\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/organ/test_brain.json\"\n",
    "\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/organ_brain/bbox_0.1/epoch9_seed4/inference/pred_beam.jsonl\"\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "# perform evaluation\n",
    "results_ours, pred_ours = evaluate(gt, pred, return_pred=True)\n",
    "\n",
    "pred_dict_ours = dict()\n",
    "for i in pred_ours:\n",
    "    pred_dict_ours[i['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the top high performance questions\n",
    "top_high_performance = [item for item in results_ours if item['f1'] > 0.5]\n",
    "top_high_performance_ids = [item['question_id'] for item in top_high_performance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_id': 12959,\n",
       "  'exact_match': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'bleu': 1.821831989445342e-231,\n",
       "  'bleu_1': 1.0,\n",
       "  'bleu_2': 2.2250738585072626e-308,\n",
       "  'bleu_3': 2.2250738585072626e-308},\n",
       " 119)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_high_performance[-1], len(top_high_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_better_ids = set(top_low_performance_ids) & set(top_high_performance_ids)\n",
    "len(our_better_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 12545\n",
      "GT: Much\n",
      "LoRA: {'question_id': 12545, 'prompt': '<image>\\nHow much damage will be caused to the body taking such an image?', 'text': 'Assistant: None', 'gt': 'Much', 'answer_id': 'YB9nAyBYhQhbVhuk6ww6mh', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Much\n",
      "\n",
      "Question ID: 12770\n",
      "GT: None\n",
      "LoRA: {'question_id': 12770, 'prompt': '<image>\\nWhat diseases are included in the picture?', 'text': 'Assistant: Brain Edema, Brain Tumor', 'gt': 'None', 'answer_id': '8jx2qChg7UGZKuQuu8tQBh', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: None\n",
      "\n",
      "Question ID: 12651\n",
      "GT: T2\n",
      "LoRA: {'question_id': 12651, 'prompt': '<image>\\nIs this a T1 weighted or T2 weighted MRI image?', 'text': 'Assistant: T1', 'gt': 'T2', 'answer_id': 'TVELXfBUQsbayHA8zZnCKE', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: T2\n",
      "\n",
      "Question ID: 12721\n",
      "GT: T1\n",
      "LoRA: {'question_id': 12721, 'prompt': '<image>\\nWhich kind of weighting is this image generated with, t1 or t2?', 'text': 'Assistant: T2', 'gt': 'T1', 'answer_id': 'JiCEJj34RimNgYREzZsWDA', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: T1\n",
      "\n",
      "Question ID: 12629\n",
      "GT: White\n",
      "LoRA: {'question_id': 12629, 'prompt': '<image>\\nWhat is the color of abnormality in this image?', 'text': 'Assistant: Gray', 'gt': 'White', 'answer_id': 'DfJwAek4gM9XgkmdF6XwzX', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: White\n",
      "\n",
      "Question ID: 12732\n",
      "GT: Left Lobe\n",
      "LoRA: {'question_id': 12732, 'prompt': '<image>\\nWhere is the brain tumor?', 'text': 'Assistant: Top', 'gt': 'Left Lobe', 'answer_id': 'HfpePHRfEULGHJnrQzhtUf', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Upper Left Lobe\n",
      "\n",
      "Question ID: 12701\n",
      "GT: Lower Right\n",
      "LoRA: {'question_id': 12701, 'prompt': '<image>\\nWhere is/are the abnormality located?', 'text': 'Assistant: Upper Left', 'gt': 'Lower Right', 'answer_id': 'C3ZQ6zxjci5SQDjWcDAwCp', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Lower Right\n",
      "\n",
      "Question ID: 12735\n",
      "GT: Hyperdense\n",
      "LoRA: {'question_id': 12735, 'prompt': '<image>\\nWhat density is the brain enhancing tumor ?', 'text': 'Assistant: T1', 'gt': 'Hyperdense', 'answer_id': 'aPRLMgnwDXBcmHh5KPnt7H', 'model_id': '/data/aofei/LLM/llava_med', 'metadata': {}}\n",
      "Ours (beam): Assistant: Hyperdense\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _id in our_better_ids:\n",
    "    print(f\"Question ID: {_id}\")\n",
    "    print(f\"GT: {pred_dict[_id]['gt']}\")\n",
    "    print(f\"LoRA: {pred_dict[_id]}\")\n",
    "    print(f\"Ours (beam): {pred_dict_ours[_id]['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
