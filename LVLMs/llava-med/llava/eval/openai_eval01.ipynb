{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " data\n",
      "num_gt_ids: 706 || num_pred_ids: 706\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data \n",
    "\n",
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "\n",
    "dir=f\"llava_med/moe_img_dense_all_query/all_expert_8_16_rank16/lora_{visual_enhance_ratio}_bbox_{bbox_ratio}/epoch{epoch_num}\"\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/test.json\"\n",
    "pred_file = f\"{ROOT_PATH}/hallucination/mitigation/{dataset}/{dir}/inference/pred.jsonl\"\n",
    "\n",
    "dataset = gt_file.split(\"/\")[-2]\n",
    "print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "gt = json.load(open(gt_file, 'r'))\n",
    "# candidate = json.load(open(args.candidate, 'r'))\n",
    "pred = load_jsonl(pred_file)\n",
    "gt = [item for item in gt if item['answer_type'] == 'OPEN']\n",
    "# pred = [item for item in pred if item['answer_type'] == 'OPEN']\n",
    "# Filter pred based on gt ids\n",
    "gt_ids = [item['id'] for item in gt]\n",
    "pred = [item for item in pred if item['question_id'] in gt_ids]\n",
    "\n",
    "\n",
    "pred_ids = [item['question_id'] for item in pred]\n",
    "num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "# import pdb; pdb.set_trace()\n",
    "assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]='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'\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_vqa_with_gpt4o_in_batches(questions, predictions, ground_truths, batch_size=10):\n",
    "    evaluation_scores = []\n",
    "\n",
    "    # Loop through questions, predictions, and ground truths in batches\n",
    "    for i in tqdm(range(0, len(predictions), batch_size)):\n",
    "        batch_questions = questions[i:i+batch_size]\n",
    "        batch_predictions = predictions[i:i+batch_size]\n",
    "        batch_ground_truths = ground_truths[i:i+batch_size]\n",
    "        \n",
    "        # Create a batch prompt\n",
    "        batch_prompt = \"\\n\".join([\n",
    "            f\"Pair {j+1}:\\nQuestion: {q}\\nPrediction: {pred}\\nGround Truth: {gt}\\n\"\n",
    "            for j, (q, pred, gt) in enumerate(zip(batch_questions, batch_predictions, batch_ground_truths))\n",
    "        ])\n",
    "        \n",
    "        full_prompt = f\"\"\"\n",
    "        Evaluate the similarity between each answer pair below based on the given question. Return a score of 0 for failure, 1 for completely correct, or 0.5 for partially correct for each pair.\\n\\n{batch_prompt}\n",
    "        Remember not to easily give a score of 0.5 if the question is asking about diagnosis or other contexts  where the answers need to be totally correct. If the answer is not correct, please provide a score of 0.\\n\\n\n",
    "      You should return a list of scores that can be directly loaded into a python list using python eval(), one for each pair, in the same order as the pairs are listed above, no explicit explanation is needed.\n",
    "\"\"\"\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant for evaluating answer accuracy.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": full_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # print(completion.choices[0].message)\n",
    "        # Parse scores from response\n",
    "        score_text = completion.choices[0].message.content.strip()\n",
    "        \n",
    "        # Convert response into a list of scores, assuming each score is on a new line\n",
    "        try:\n",
    "            batch_scores = eval(score_text)\n",
    "            evaluation_scores.extend(batch_scores)\n",
    "        except ValueError:\n",
    "            print(f\"Unexpected response format: {score_text}\")\n",
    "            evaluation_scores.extend([0] * len(batch_predictions))  # Default to 0 if parsing fails\n",
    "\n",
    "    return evaluation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_answers = []\n",
    "pred_answers = []\n",
    "questions = []\n",
    "for item in pred:\n",
    "    pred_item = item['text']\n",
    "    if item['text'].startswith(\"Assistant:\"):\n",
    "        pred_item = item['text'][10:].strip()\n",
    "    gt_item = item[\"gt\"]\n",
    "    question = item[\"prompt\"].strip(\"<image>\\n\")\n",
    "    gt_answers.append(gt_item)\n",
    "    pred_answers.append(pred_item)\n",
    "    questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:57<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores: 0.7662889518413598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_vqa_with_gpt4o_in_batches(questions, gt_answers, pred_answers, batch_size=10)\n",
    "print(\"Evaluation Scores:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'xmlab102/source.jpg',\n",
       " 'id': 11934,\n",
       " 'organ': 'Lung',\n",
       " 'answer_type': 'OPEN',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat modality is used to take this image?'},\n",
       "  {'from': 'gpt', 'value': 'CT'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_answers(gt_file, pred_file):\n",
    "    dataset = gt_file.split(\"/\")[-2]\n",
    "    print(f\"\\n========\\n {dataset}\")\n",
    "\n",
    "    gt = json.load(open(gt_file, 'r'))\n",
    "    # candidate = json.load(open(args.candidate, 'r'))\n",
    "    pred = load_jsonl(pred_file)\n",
    "    gt = [item for item in gt if item['answer_type'] == 'OPEN']\n",
    "    # pred = [item for item in pred if item['answer_type'] == 'OPEN']\n",
    "    # Filter pred based on gt ids\n",
    "    gt_ids = [item['id'] for item in gt]\n",
    "    pred = [item for item in pred if item['question_id'] in gt_ids]\n",
    "\n",
    "\n",
    "    pred_ids = [item['question_id'] for item in pred]\n",
    "    num_gt_ids, num_pred_ids = len(gt_ids), len(pred_ids)\n",
    "    print(f'num_gt_ids: {num_gt_ids} || num_pred_ids: {num_pred_ids}')\n",
    "    # import pdb; pdb.set_trace()\n",
    "    assert gt_ids == pred_ids, \"please make sure pred and gt are exactly matched\"\n",
    "\n",
    "    gt_answers = []\n",
    "    pred_answers = []\n",
    "    questions = []\n",
    "    for i in range(len(pred)):\n",
    "        item = pred[i]\n",
    "        pred_item = item['text']\n",
    "        if item['text'].startswith(\"Assistant:\"):\n",
    "            pred_item = item['text'][10:].strip()\n",
    "        if item.__contains__(\"gt\"):\n",
    "            gt_item = item[\"gt\"]\n",
    "        else:\n",
    "            gt_item = gt[i][\"conversations\"][1][\"value\"]\n",
    "        question = item[\"prompt\"].strip(\"<image>\\n\")\n",
    "        gt_answers.append(gt_item)\n",
    "        pred_answers.append(pred_item)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions, gt_answers, pred_answers\n",
    "\n",
    "def eval_openai(questions, gt_answers, pred_answers):\n",
    "    scores = evaluate_vqa_with_gpt4o_in_batches(questions, gt_answers, pred_answers, batch_size=10)\n",
    "    print(\"Evaluation Scores:\", sum(scores) / len(scores))\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========\n",
      " data\n",
      "num_gt_ids: 706 || num_pred_ids: 706\n"
     ]
    }
   ],
   "source": [
    "visual_enhance_ratio=0.08\n",
    "bbox_ratio=0.03\n",
    "epoch_num=6\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "dataset=\"Slake\"\n",
    "dir=f\"llava_med/moe_img_dense_all_query/all_expert_8_16_rank16/lora_{visual_enhance_ratio}_bbox_{bbox_ratio}/epoch{epoch_num}\"\n",
    "gt_file = f\"{ROOT_PATH}/hallucination/{dataset}/data/test.json\"\n",
    "# pred_file = f\"{ROOT_PATH}/hallucination/mitigation/{dataset}/{dir}/inference/pred.jsonl\"\n",
    "pred_file = \"/data/aofei/hallucination/mitigation/Slake/llava_med/lora/epoch6/inference/pred.jsonl\"\n",
    "\n",
    "questions, gt_answers, pred_answers = eval_openai(gt_file, pred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:54<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores: 0.7868271954674221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_scores = eval_openai(questions, gt_answers, pred_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
