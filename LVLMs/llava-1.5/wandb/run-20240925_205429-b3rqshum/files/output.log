  0%|                                                                                                                                                                                   | 0/675 [00:00<?, ?it/s]/data/aofei/conda/env/llava_v1.5/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/aofei/conda/env/llava_v1.5/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2422, -0.1279,  0.1138,  ...,  0.0771, -0.1187,  0.1445],
         [ 0.4336, -0.7852, -2.2969,  ...,  1.0547,  1.6719,  0.4629],
         [ 1.0469,  0.9688,  1.3516,  ..., -1.0625, -0.7461,  0.1523],
         ...,
         [ 0.9609,  0.0142,  1.9141,  ..., -0.9688,  3.7656, -1.0234],
         [ 1.1406, -0.0427,  1.8359,  ..., -0.6172,  0.8164, -1.1875],
         [ 0.2236, -0.7383,  0.8828,  ..., -1.2656,  0.1504, -0.9648]],

        [[ 0.2422, -0.1279,  0.1138,  ...,  0.0771, -0.1187,  0.1445],
         [ 0.4336, -0.7852, -2.2969,  ...,  1.0547,  1.6719,  0.4629],
         [ 1.0469,  0.9688,  1.3516,  ..., -1.0625, -0.7461,  0.1523],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
/data/aofei/conda/env/llava_v1.5/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|â–Œ                                                                                                                                                                          | 2/675 [00:03<19:57,  1.78s/it]
{'loss': 4.5296, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.0}
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2422, -0.1279,  0.1138,  ...,  0.0771, -0.1187,  0.1445],
         [ 0.4336, -0.7852, -2.2969,  ...,  1.0547,  1.6719,  0.4629],
         [ 1.0469,  0.9688,  1.3516,  ..., -1.0625, -0.7461,  0.1523],
         ...,
         [ 1.7812, -2.2812,  0.8320,  ...,  0.0483,  1.2969, -0.2236],
         [-0.1309, -0.7734,  1.6016,  ...,  0.2178, -0.5000, -0.1426],
         [ 0.7422, -0.0583,  2.3438,  ..., -0.5508,  0.0381, -0.8320]],

        [[ 0.2422, -0.1279,  0.1138,  ...,  0.0771, -0.1187,  0.1445],
         [ 0.4336, -0.7852, -2.2969,  ...,  1.0547,  1.6719,  0.4629],
         [ 1.0469,  0.9688,  1.3516,  ..., -1.0625, -0.7461,  0.1523],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
{'loss': 6.2309, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.01}
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2393, -0.1211,  0.1147,  ...,  0.0791, -0.1143,  0.1436],
         [ 0.4336, -0.7891, -2.2812,  ...,  1.0391,  1.6719,  0.4688],
         [ 1.0469,  0.9766,  1.3516,  ..., -1.0391, -0.7773,  0.1777],
         ...,
         [ 1.9531, -1.2969,  0.6133,  ..., -0.8398,  3.8594, -1.2891],
         [ 2.1875, -0.8008,  0.0718,  ...,  0.3848,  3.5781,  0.2207],
         [ 0.3398, -0.5234, -0.0864,  ..., -0.2148,  0.8320, -1.3203]],

        [[ 0.2402, -0.1250,  0.1138,  ...,  0.0781, -0.1177,  0.1455],
         [ 0.4297, -0.7852, -2.2969,  ...,  1.0547,  1.6641,  0.4590],
         [ 1.0469,  0.9414,  1.3516,  ..., -1.0703, -0.7891,  0.1748],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
{'loss': 5.7727, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.01}
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2354, -0.1211,  0.1138,  ...,  0.0791, -0.1123,  0.1445],
         [ 0.4453, -0.7891, -2.2812,  ...,  1.0312,  1.6719,  0.4590],
         [ 1.0469,  0.9180,  1.3359,  ..., -1.0391, -0.7695,  0.1748],
         ...,
         [ 0.0520, -1.3750,  3.5156,  ..., -1.5859,  0.6641, -1.2109],
         [ 1.6328, -0.7539,  2.5312,  ..., -0.4727,  1.1484, -0.2539],
         [ 0.9922, -1.3750,  2.2188,  ..., -1.1406,  0.2969, -1.2734]],

        [[ 0.2354, -0.1211,  0.1123,  ...,  0.0801, -0.1123,  0.1445],
         [ 0.4277, -0.7969, -2.2969,  ...,  1.0156,  1.6484,  0.4590],
         [ 1.0469,  0.9609,  1.3359,  ..., -1.0547, -0.7734,  0.1826],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
{'loss': 7.1381, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.02}
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2324, -0.1221,  0.1138,  ...,  0.0806, -0.1084,  0.1475],
         [ 0.4238, -0.7930, -2.2656,  ...,  1.0234,  1.6562,  0.4629],
         [ 1.0469,  0.8984,  1.3125,  ..., -1.0469, -0.7812,  0.1855],
         ...,
         [ 0.7266,  0.1187, -0.4492,  ..., -1.9453, -0.7227, -1.4688],
         [ 1.3750, -1.1016,  0.2852,  ..., -2.8281,  0.3711, -1.2188],
         [ 0.1436,  0.8516,  1.5625,  ..., -1.8672,  1.3281, -1.6016]],

        [[ 0.2295, -0.1201,  0.1123,  ...,  0.0806, -0.1089,  0.1465],
         [ 0.4160, -0.7852, -2.3281,  ...,  1.0234,  1.6641,  0.4590],
         [ 1.0469,  0.9336,  1.3125,  ..., -1.0391, -0.7734,  0.1826],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
{'loss': 5.5887, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.02}
odict_keys(['last_hidden_state', 'attentions']) BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.2334, -0.1201,  0.1118,  ...,  0.0859, -0.1050,  0.1514],
         [ 0.4160, -0.7812, -2.3125,  ...,  1.0156,  1.6641,  0.4668],
         [ 1.0391,  0.9414,  1.2891,  ..., -1.0469, -0.7500,  0.2021],
         ...,
         [ 0.5273, -0.1621,  2.4062,  ..., -0.9336,  0.2275,  0.4375],
         [-0.2734, -1.3125,  2.5000,  ..., -1.7734,  1.0938, -0.9688],
         [ 0.0718, -0.3379,  2.0000,  ..., -1.2422,  0.0388, -0.4355]],

        [[ 0.2334, -0.1187,  0.1128,  ...,  0.0845, -0.1060,  0.1523],
         [ 0.4082, -0.7734, -2.2812,  ...,  1.0156,  1.6406,  0.4688],
         [ 1.0391,  0.9414,  1.2812,  ..., -1.0391, -0.7773,  0.2188],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
