{
  "os": "Linux-5.15.0-117-generic-x86_64-with-glibc2.35",
  "python": "3.10.14",
  "startedAt": "2024-09-28T19:32:12.275967Z",
  "args": [
    "--local_rank=0",
    "--lora_enable",
    "True",
    "--lora_r",
    "128",
    "--lora_alpha",
    "256",
    "--mm_projector_lr",
    "2e-5",
    "--deepspeed",
    "./scripts/zero2.json",
    "--model_name_or_path",
    "/data/aofei/LLM/llava-v1.5",
    "--version",
    "v1",
    "--data_path",
    "/data/aofei/hallucination/VQA_RAD/data/training_bboxes.json",
    "--image_folder",
    "/data/aofei/hallucination/VQA_RAD/images",
    "--vision_tower",
    "openai/clip-vit-large-patch14-336",
    "--pretrain_mm_mlp_adapter",
    "/data/aofei/LLM/llava-v1.5/mm_projector.bin",
    "--mm_projector_type",
    "mlp2x_gelu",
    "--mm_vision_select_layer",
    "-2",
    "--mm_use_im_start_end",
    "False",
    "--mm_use_im_patch_token",
    "False",
    "--image_aspect_ratio",
    "pad",
    "--group_by_modality_length",
    "True",
    "--bf16",
    "True",
    "--output_dir",
    "/data/aofei/hallucination/mitigation/VQA_RAD/llava_1.5/lora_bbox/epoch_test2_flash/checkpoints",
    "--num_train_epochs",
    "3",
    "--per_device_train_batch_size",
    "1",
    "--per_device_eval_batch_size",
    "4",
    "--gradient_accumulation_steps",
    "1",
    "--evaluation_strategy",
    "no",
    "--save_strategy",
    "steps",
    "--save_steps",
    "500",
    "--save_total_limit",
    "1",
    "--learning_rate",
    "2e-4",
    "--weight_decay",
    "0.",
    "--warmup_ratio",
    "0.03",
    "--lr_scheduler_type",
    "cosine",
    "--logging_steps",
    "1",
    "--tf32",
    "True",
    "--model_max_length",
    "2048",
    "--gradient_checkpointing",
    "True",
    "--dataloader_num_workers",
    "4",
    "--lazy_preprocess",
    "True",
    "--report_to",
    "wandb",
    "--use_bbox",
    "True"
  ],
  "program": "/home/avc6555/research/MedH/Mitigation/LVLMs/llava-1.5/llava/train/train_mem.py",
  "codePath": "llava/train/train_mem.py",
  "email": "lunnarbeing@gmail.com",
  "root": "/home/avc6555/research/MedH/Mitigation/LVLMs/llava-1.5",
  "host": "i4-l-ffm5105-01",
  "username": "avc6555",
  "executable": "/data/aofei/conda/env/llava_v1.5/bin/python",
  "codePathLocal": "llava/train/train_mem.py",
  "cpu_count": 24,
  "cpu_count_logical": 48,
  "gpu": "[NVIDIA RTX A6000, NVIDIA RTX A6000, NVIDIA RTX A6000, NVIDIA RTX A6000]",
  "gpu_count": 4,
  "disk": {
    "/": {
      "total": "210241560576",
      "used": "151226757120"
    }
  },
  "memory": {
    "total": "404327792640"
  },
  "cpu": {
    "count": 24,
    "countLogical": 48
  },
  "gpu_nvidia": [
    {
      "name": "NVIDIA RTX A6000",
      "memoryTotal": "51527024640",
      "cudaCores": 10752,
      "architecture": "Ampere"
    },
    {
      "name": "NVIDIA RTX A6000",
      "memoryTotal": "51527024640",
      "cudaCores": 10752,
      "architecture": "Ampere"
    },
    {
      "name": "NVIDIA RTX A6000",
      "memoryTotal": "51527024640",
      "cudaCores": 10752,
      "architecture": "Ampere"
    },
    {
      "name": "NVIDIA RTX A6000",
      "memoryTotal": "51527024640",
      "cudaCores": 10752,
      "architecture": "Ampere"
    }
  ],
  "cudaVersion": "12.6"
}