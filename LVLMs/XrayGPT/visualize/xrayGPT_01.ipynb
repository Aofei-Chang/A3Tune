{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/minigptv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from visualizer import get_local\n",
    "get_local.activate()\n",
    "\n",
    "import os\n",
    "ROOT_PATH=\"/data/aofei\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{ROOT_PATH}/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_HOME\"] = f\"{ROOT_PATH}/huggingface_cache/transformers\"\n",
    "os.environ[\"TORCH_HOME\"] = f\"{ROOT_PATH}/huggingface_cache/torch\"\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/avc6555/research/MedH/Mitigation/LVLMs/XrayGPT\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from xraygpt.common.config import Config\n",
    "from xraygpt.common.dist_utils import get_rank\n",
    "from xraygpt.common.registry import registry\n",
    "from xraygpt.conversation.conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from xraygpt.datasets.builders import *\n",
    "from xraygpt.models import *\n",
    "from xraygpt.processors import *\n",
    "from xraygpt.runners import *\n",
    "from xraygpt.tasks import *\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prompts/alignment.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model_config\u001b[38;5;241m.\u001b[39mdevice_8bit \u001b[38;5;241m=\u001b[39m gpu_id\n\u001b[1;32m     12\u001b[0m model_cls \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mget_model_class(model_config\u001b[38;5;241m.\u001b[39march)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# CONV_VISION = conv_dict[model_config.model_type]\u001b[39;00m\n",
      "File \u001b[0;32m~/research/MedH/Mitigation/LVLMs/XrayGPT/xraygpt/models/mini_gpt4.py:358\u001b[0m, in \u001b[0;36mMiniGPT4.from_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m    355\u001b[0m max_txt_len \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_txt_len\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    356\u001b[0m end_sym \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_sym\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 358\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_former_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_former_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_grad_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_grad_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvit_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_vit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_vit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_qformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_qformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_query_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_query_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_txt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_txt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_sym\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_sym\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load weights of MiniGPT-4\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ckpt_path:\n",
      "File \u001b[0;32m~/research/MedH/Mitigation/LVLMs/XrayGPT/xraygpt/models/mini_gpt4.py:134\u001b[0m, in \u001b[0;36mMiniGPT4.__init__\u001b[0;34m(self, vit_model, q_former_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision, freeze_vit, freeze_qformer, num_query_token, llama_model, prompt_path, prompt_template, max_txt_len, low_resource, end_sym)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_sym \u001b[38;5;241m=\u001b[39m end_sym\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_path:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    135\u001b[0m         raw_prompts \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    136\u001b[0m     filted_prompts \u001b[38;5;241m=\u001b[39m [raw_prompt \u001b[38;5;28;01mfor\u001b[39;00m raw_prompt \u001b[38;5;129;01min\u001b[39;00m raw_prompts \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<ImageHere>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_prompt]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prompts/alignment.txt'"
     ]
    }
   ],
   "source": [
    "gpu_id = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cfg_path=\"/home/avc6555/research/MedH/Mitigation/LVLMs/XrayGPT/eval_configs/xraygpt_eval.yaml\"\n",
    "        self.options=[]\n",
    "\n",
    "args = Args()\n",
    "\n",
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda')\n",
    "device = torch.device('cuda')\n",
    "# CONV_VISION = conv_dict[model_config.model_type]\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.openi.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "stop_words_ids = [torch.tensor([835]).to(device),\n",
    "                    torch.tensor([2277, 29937]).to(device)]  # '###' can be encoded in two different ways.\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "\n",
    "prompt = conv.get_prompt()\n",
    "prompt_segs = prompt.split('<ImageHere>')\n",
    "seg_tokens = [\n",
    "    model.llama_tokenizer(\n",
    "        seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(model.device).input_ids\n",
    "    # only add bos to the first seg\n",
    "    for i, seg in enumerate(prompt_segs)\n",
    "]\n",
    "[i.size() for i in seg_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_emb(conv, img_list, model):\n",
    "    prompt = conv.get_prompt()\n",
    "    prompt_segs = prompt.split('<ImageHere>')\n",
    "    seg_tokens = [\n",
    "        model.llama_tokenizer(\n",
    "            seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(model.device).input_ids\n",
    "        # only add bos to the first seg\n",
    "        for i, seg in enumerate(prompt_segs)\n",
    "    ]\n",
    "    seg_embs = [model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "image = \"/home/avc6555/research/MedH/Mitigation/LVLMs/llava-med/visualize/images/cat.jpg\"\n",
    "question = \"What is the abnormity in this image?\"\n",
    "# question = \"What are the main findings and impression of the given x-ray?\"\n",
    "conv = CONV_VISION\n",
    "\n",
    "if isinstance(image, str):  # is a image path\n",
    "    raw_image = Image.open(image).convert('RGB')\n",
    "    image = vis_processor(raw_image).unsqueeze(0).to(device)\n",
    "elif isinstance(image, Image.Image):\n",
    "    raw_image = image\n",
    "    image = vis_processor(raw_image).unsqueeze(0).to(device)\n",
    "elif isinstance(image, torch.Tensor):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "img_list = []\n",
    "image_emb, _ = model.encode_img(image)\n",
    "img_list.append(image_emb)\n",
    "conv.messages = []\n",
    "conv.append_message(conv.roles[0], \"<Img><ImageHere></Img>\")\n",
    "# question = \"Could you highlight any abnormalities or concerns in this chest x-ray image?\"\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "embs = get_context_emb(conv, img_list, model)\n",
    "\n",
    "begin_idx = 0\n",
    "\n",
    "embs = embs[:, begin_idx:]\n",
    "\n",
    "outputs = model.llama_model.generate(\n",
    "        inputs_embeds=embs,\n",
    "        max_new_tokens=64,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        num_beams=1,\n",
    "        do_sample=True,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1,\n",
    "        length_penalty=1,\n",
    "        temperature=1,\n",
    "        output_attentions=True, return_dict_in_generate=True\n",
    "    )\n",
    "output_token = outputs['sequences'][0]\n",
    "if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "    output_token = output_token[1:]\n",
    "if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "    output_token = output_token[1:]\n",
    "output_text = model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "output_text = output_text.split('Doctor:')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
