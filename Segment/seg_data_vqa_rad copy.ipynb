{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import clip\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide \n",
    "\n",
    "from PIL import Image  \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from sam_caf import hyper_params_tuning, get_crops, retrieve_relevant_crop, retrieve_relevant_crop_biomed, get_sam_prompts, sam_predicton, retrieve_relevant_crop_biomed_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "class DictToObject:\n",
    "    def __init__(self, dict_obj):\n",
    "        for key, value in dict_obj.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config_dict = {\n",
    "    \"model_name\" : \"SAM\",\n",
    "    \"model_type\" : \"vit_h\",\n",
    "    \"source\":    \"False\", \n",
    "    \"refine\" : \"False\",\n",
    "    \"pre_trained\": \"True\", \n",
    "    \"sam_ckpt\":  \"/data/aofei/LLM/SAM/sam_vit_h_4b8939.pth\", \n",
    "    \"clip_prompts\": \"./clip_prompts/abd_seg.json\"\n",
    "}\n",
    "\n",
    "config = DictToObject(config_dict)\n",
    "\n",
    "prompt_mode, mode = \"crops\", \"sam_clip\"\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/aofei/conda/env/medh/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"]=\"/data/aofei/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_HOME\"]=\"/data/aofei/huggingface_cache/transformers\"\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "biomed_clip_model, biomed_preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', device=\"cuda\")\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=\"cuda\")\n",
    "sam_checkpoint = config.sam_ckpt\n",
    "\n",
    "sam = sam_model_registry[config.model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(\"cuda\")\n",
    "resize_transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
    "\n",
    "dice_scores = []\n",
    "mask_generator, area = hyper_params_tuning(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_generation(image_path):\n",
    "    image = preprocess_image(image_path=image_path)\n",
    "    with torch.no_grad():\n",
    "        # if mode == \"sam_clip\":\n",
    "        masks = mask_generator.generate(image)\n",
    "        masks = [mask for mask in masks if mask[\"area\"] < area] # area filtering based on area value from hyper-params tuning\n",
    "        img_crops = get_crops(image, masks, prompt_mode)\n",
    "        \n",
    "    return masks, img_crops\n",
    "\n",
    "def filter_sam_results(masks, img_crops):\n",
    "    new_masks, new_img_crops = [], []\n",
    "    for i in range(len(masks)):\n",
    "        mask = masks[i]\n",
    "        if mask['bbox'][0] == 0 or mask['bbox'][1] == 0:\n",
    "            continue\n",
    "        if mask['bbox'][2] <= 12 or mask['bbox'][3] <= 12:\n",
    "            continue\n",
    "        y_max, x_max = mask['bbox'][1] + mask['bbox'][3], mask['bbox'][0] + mask['bbox'][2]\n",
    "        if y_max > 253 or x_max > 253:\n",
    "            continue\n",
    "        new_masks.append(mask)\n",
    "        new_img_crops.append(img_crops[i])\n",
    "    return new_masks, new_img_crops\n",
    "\n",
    "def get_topk_similar(k, crop_scores):\n",
    "    sorted_scores = sorted([(i, m) for (i,m) in enumerate(crop_scores)], key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:k]\n",
    "\n",
    "def get_compelete_contour(masks):\n",
    "    width_list = []\n",
    "    # need to consider the chest xray\n",
    "    for i in masks:\n",
    "        width, height = i['bbox'][2], i['bbox'][3]\n",
    "        width_list.append(width)\n",
    "    sorted_width = sorted([(i, m) for (i,m) in  enumerate(width_list)], key=lambda x: x[1], reverse=True)\n",
    "    return sorted_width[0][1]\n",
    "\n",
    "def judge_inner_boxes(bboxes):\n",
    "    for bbox in bboxes:\n",
    "        bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_image_dict = dict()\n",
    "masks_all_image_dict = dict()\n",
    "crops_image_dict = dict()\n",
    "def generate_segments(query, image_path):\n",
    "    if masks_image_dict.__contains__(image_path):\n",
    "        masks = masks_image_dict[image_path]\n",
    "        img_crops = crops_image_dict[image_path]\n",
    "    else:\n",
    "        masks, img_crops = sam_generation(image_path=image_path)\n",
    "        masks, img_crops = filter_sam_results(masks, img_crops)\n",
    "        masks_image_dict[image_path] = masks\n",
    "        crops_image_dict[image_path] = img_crops\n",
    "    img_crops_filtered = img_crops\n",
    "    prompts = {\"query\": [query]}\n",
    "    max_indices, scores = retrieve_relevant_crop_biomed_topk(img_crops, prompts, biomed_clip_model, biomed_preprocess, config, tokenizer=tokenizer, topk=8)\n",
    "    # topk_indices = get_topk_similar(3, scores[\"query\"])\n",
    "    # define a set of rules, firstly return top3\n",
    "    # if there is no explicit organs to be used as query, then just use the whole segmentation\n",
    "    # if the smaller boxes are in the bigger box, then use all of them but assign higher weights on smaller inner boxes\n",
    "    bboxes = []\n",
    "    segs = []\n",
    "    # print(max_indices)\n",
    "    for i in max_indices[\"query\"]:\n",
    "        bboxes.append(masks[i][\"bbox\"])\n",
    "        segs.append(masks[i][\"segmentation\"])\n",
    "    return bboxes, segs, max_indices[\"query\"]\n",
    "\n",
    "\n",
    "def generate_all_segments(image_path):\n",
    "    if masks_all_image_dict.__contains__(image_path):\n",
    "        masks = masks_all_image_dict[image_path]\n",
    "    else:\n",
    "        masks, img_crops = sam_generation(image_path=image_path)\n",
    "        masks, img_crops = filter_sam_results(masks, img_crops)\n",
    "        masks_all_image_dict[image_path] = masks\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Slake\n",
    "# import json\n",
    "# # with open(r\"/data/aofei/hallucination/Slake/data/training_contours.json\", \"r\") as f:\n",
    "# #     train_data = json.load(f)\n",
    "# # len(train_data)\n",
    "\n",
    "# with open(r\"/data/aofei/hallucination/Slake/train.json\", \"r\") as f:\n",
    "#     all_train_data = json.load(f)\n",
    "# len(all_train_data)\n",
    "\n",
    "# with open(r\"/data/aofei/hallucination/Slake/data/test.json\", \"r\") as f:\n",
    "#     test_data = json.load(f)\n",
    "# len(test_data)\n",
    "\n",
    "# with open(r\"/data/aofei/hallucination/Slake/test.json\", \"r\") as f:\n",
    "#     all_test_data = json.load(f)\n",
    "# len(all_test_data)\n",
    "\n",
    "# Preprocessing for VQA-RAD\n",
    "import json\n",
    "with open(r\"/data/aofei/hallucination/VQA_RAD/MED_RAD_test.json\", \"r\") as f:\n",
    "    rad_data = json.load(f)\n",
    "len(rad_data)\n",
    "\n",
    "train_rad_data = []\n",
    "for i in rad_data:\n",
    "    if not i['phrase_type'].startswith(\"test\"):\n",
    "        train_rad_data.append(i)\n",
    "len(train_rad_data)\n",
    "\n",
    "all_train_data = train_rad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': '0',\n",
       " 'phrase_type': 'freeform',\n",
       " 'qid_linked_id': '03f451ca-de62-4617-9679-e836026a7642',\n",
       " 'image_case_url': 'https://medpix.nlm.nih.gov/case?id=48e1dd0e-8552-46ad-a354-5eb55be86de6',\n",
       " 'image_name': 'synpic54610.jpg',\n",
       " 'image_organ': 'HEAD',\n",
       " 'evaluation': 'not evaluated',\n",
       " 'question': 'Are regions of the brain infarcted?',\n",
       " 'question_rephrase': 'NULL',\n",
       " 'question_relation': 'NULL',\n",
       " 'question_frame': 'NULL',\n",
       " 'question_type': 'PRES',\n",
       " 'answer': 'Yes',\n",
       " 'answer_type': 'CLOSED'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_en = all_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "train_data_seg = copy.deepcopy(all_train_data_en)\n",
    "len(train_data_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([i['image_name'] for i in train_data_seg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(set([i['img_name'] for i in train_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1797 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 29/1797 [00:22<32:56,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping zero-sized bounding box.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 46/1797 [00:45<40:47,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping zero-sized bounding box.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 227/1797 [02:31<13:41,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping zero-sized bounding box.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 957/1797 [11:07<08:13,  1.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping zero-sized bounding box.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1797/1797 [16:03<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(train_rad_data)):\n",
    "#failure case: 432-3 + 100\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(train_data_seg))):\n",
    "# for i in tqdm(range(20)):\n",
    "    data = train_data_seg[i]\n",
    "    image_path = os.path.join(\"/data/aofei/hallucination/VQA_RAD/images\", data[\"image_name\"])\n",
    "    if \"chest\" in data['image_organ'].lower():\n",
    "        # query = f\"Medical image of {data['image_organ']} and lungs. \" + data[\"question\"]\n",
    "        # query = f\"{data['location']}\"\n",
    "        query = f\"Medical image of {data['image_organ']} and lungs. \" + data[\"question\"]\n",
    "    elif \"abd\" in data['image_organ'].lower():\n",
    "        query = f\"Medical imgaing of abdomen CT.\" + data[\"question\"]\n",
    "        # query = f\"Medical imgaing of abdomen.\"\n",
    "    else:\n",
    "        # query = f\"Medical image of {data['image_organ']}. \" + data[\"question\"]\n",
    "        query = f\"Medical image of {data['image_organ']}.\" + data[\"question\"]\n",
    "    bbox, segs, max_indices = [], [], []\n",
    "    # try:\n",
    "    bbox, segs, max_indices = generate_segments(query, image_path)\n",
    "    # except:\n",
    "    #     continue\n",
    "    # print(bbox)\n",
    "    data['bbox'] = bbox\n",
    "    data['mask'] = segs\n",
    "    data[\"bbox_indices\"] = max_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': '0',\n",
       " 'phrase_type': 'freeform',\n",
       " 'qid_linked_id': '03f451ca-de62-4617-9679-e836026a7642',\n",
       " 'image_case_url': 'https://medpix.nlm.nih.gov/case?id=48e1dd0e-8552-46ad-a354-5eb55be86de6',\n",
       " 'image_name': 'synpic54610.jpg',\n",
       " 'image_organ': 'HEAD',\n",
       " 'evaluation': 'not evaluated',\n",
       " 'question': 'Are regions of the brain infarcted?',\n",
       " 'question_rephrase': 'NULL',\n",
       " 'question_relation': 'NULL',\n",
       " 'question_frame': 'NULL',\n",
       " 'question_type': 'PRES',\n",
       " 'answer': 'Yes',\n",
       " 'answer_type': 'CLOSED',\n",
       " 'bbox': [[22, 13, 189, 217],\n",
       "  [23, 17, 100, 212],\n",
       "  [103, 15, 107, 208],\n",
       "  [55, 130, 26, 26]],\n",
       " 'mask': [array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]])],\n",
       " 'bbox_indices': [1, 3, 2, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for i in train_data_seg:\n",
    "    if not i.__contains__(\"mask\"):\n",
    "        s+= 1\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 3,\n",
       " 'phrase_type': 'freeform',\n",
       " 'qid_linked_id': '0e90b6bc-265f-490b-a039-509b9907a3cb',\n",
       " 'image_case_url': 'https://medpix.nlm.nih.gov/case?id=19aa8a2b-35fb-4d90-973d-ccc3859df66e',\n",
       " 'image_name': 'synpic28602.jpg',\n",
       " 'image_organ': 'CHEST',\n",
       " 'evaluation': 'given',\n",
       " 'question': 'What type of imaging does this not represent?',\n",
       " 'question_rephrase': 'NULL',\n",
       " 'question_relation': 'NULL',\n",
       " 'question_frame': 'NULL',\n",
       " 'question_type': 'MODALITY',\n",
       " 'answer': 'ultrasound',\n",
       " 'answer_type': 'OPEN',\n",
       " 'bbox': [[70, 6, 28, 19],\n",
       "  [135, 52, 89, 131],\n",
       "  [166, 197, 30, 19],\n",
       "  [70, 6, 95, 19],\n",
       "  [32, 65, 70, 128],\n",
       "  [224, 12, 21, 13],\n",
       "  [2, 26, 81, 27],\n",
       "  [135, 6, 30, 17]],\n",
       " 'mask': [array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]),\n",
       "  array([[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]])],\n",
       " 'bbox_indices': [3, 6, 5, 2, 1, 8, 4, 0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_seg[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'synpic56841.jpg',\n",
       " 'id': 2242,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nIs this a T1 weighted, T2 weighted, or FLAIR image?'},\n",
       "  {'from': 'gpt', 'value': 'FLAIR'}],\n",
       " 'bboxes': [[23, 8, 184, 226],\n",
       "  [100, 145, 29, 33],\n",
       "  [167, 73, 20, 31],\n",
       "  [176, 83, 24, 25]],\n",
       " 'masks': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training2\n",
    "new_train_data = []\n",
    "segments_dict = dict()\n",
    "for i in train_data_seg:\n",
    "    template = dict()\n",
    "    \n",
    "    # template['answer_type'] = i['answer_type']\n",
    "    template['image'] = i['image_name']\n",
    "    template['id'] = i['qid']\n",
    "    template['conversations'] = []\n",
    "    template['bboxes'] = []\n",
    "    template['masks'] = []\n",
    "    segments_dict[str(i['qid'])] = []\n",
    "    if i.__contains__(\"bbox\"):\n",
    "        template['bboxes'] = i[\"bbox\"]\n",
    "    if i.__contains__(\"mask\"):\n",
    "        segments_dict[str(i['qid'])] = i[\"mask\"]\n",
    "\n",
    "    # if i.__contains__(\"mask\"):\n",
    "    #     json_ready_segments = [\n",
    "    #         arr.astype(int).tolist() for arr in i[\"mask\"]\n",
    "    #     ]\n",
    "    #     template['masks'] = json_ready_segments\n",
    "    # template['text'] = i['question']\n",
    "\n",
    "    new_qa = {\"from\": \"human\", \"value\": \"<image>\\n\" + i['question']}\n",
    "    new_qa2 = {\"from\": \"gpt\", \"value\": str(i['answer'])}\n",
    "    template['conversations'] += [new_qa, new_qa2]\n",
    "    new_train_data.append(template)\n",
    "\n",
    "new_train_data[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data_top4 = []\n",
    "for i in new_train_data:\n",
    "    j = i.copy()\n",
    "    j[\"bboxes\"] = j[\"bboxes\"][:4]\n",
    "    new_train_data_top4.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'synpic31232.jpg',\n",
       " 'id': 2247,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat is the hypo-dense area seen in the vertebrae?'},\n",
       "  {'from': 'gpt', 'value': 'Nucleus Pulposus'}],\n",
       " 'bboxes': [[105, 147, 61, 79],\n",
       "  [78, 147, 88, 90],\n",
       "  [26, 32, 195, 205],\n",
       "  [85, 145, 25, 50]],\n",
       " 'masks': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data_top4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segments_dict['0']\n",
    "ed = 0\n",
    "for i in segments_dict:\n",
    "    if len(segments_dict[i]) == 0:\n",
    "        ed += 1\n",
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the masks to npz file\n",
    "\n",
    "np.savez_compressed(\"/data/aofei/hallucination/VQA_RAD/data/training_segments_top8.npz\", **segments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "loaded_segments = np.load(\"/data/aofei/hallucination/Slake/data/training_segments.npz\", allow_pickle=True)  # Allow pickle to handle lists\n",
    "\n",
    "# Access a list of segment arrays by its ID, e.g., \"id_1\"\n",
    "segments_list_id_1 = loaded_segments[\"0\"]\n",
    "\n",
    "# Each item in segments_list_id_1 is a 256x256 numpy array\n",
    "for segment in segments_list_id_1:\n",
    "    print(segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/aofei/hallucination/VQA_RAD/data/training_masks_top4.json', 'w') as json_file:\n",
    "    json.dump(new_train_data_top4, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/aofei/hallucination/VQA_RAD/data/training_masks_top8.json', 'w') as json_file:\n",
    "    json.dump(new_train_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/data/aofei/hallucination/VQA-RAD/test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "len(test_data)\n",
    "\n",
    "#training2\n",
    "new_test_data = []\n",
    "for i in test_data:\n",
    "    if i['q_lang'] != \"en\":\n",
    "        continue\n",
    "    template = dict()\n",
    "    \n",
    "    # template['answer_type'] = i['answer_type']\n",
    "    template['image'] = i['img_name']\n",
    "    template['id'] = i['qid']\n",
    "    template['answer_type'] = i['answer_type']\n",
    "    template['conversations'] = []\n",
    "\n",
    "    new_qa = {\"from\": \"human\", \"value\": \"<image>\\n\" + i['question']}\n",
    "    new_qa2 = {\"from\": \"gpt\", \"value\": str(i['answer'])}\n",
    "    template['conversations'] += [new_qa, new_qa2]\n",
    "    new_test_data.append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))\n",
    "with open('/data/aofei/hallucination/Slake/data/test.json', 'w') as json_file:\n",
    "    json.dump(new_test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"/data/aofei/hallucination/Slake/data/training_masks_top8.json\", \"r\") as f:\n",
    "    seg_train_data = json.load(f)\n",
    "len(seg_train_data)\n",
    "seg_train_dict = dict()\n",
    "for i in seg_train_data:\n",
    "    seg_train_dict[i['id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9835"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"/data/aofei/hallucination/Slake/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_id': 1,\n",
       " 'img_name': 'xmlab1/source.jpg',\n",
       " 'question': 'Which part of the body does this image belong to?',\n",
       " 'answer': 'Abdomen',\n",
       " 'q_lang': 'en',\n",
       " 'location': 'Abdomen',\n",
       " 'modality': 'MRI',\n",
       " 'answer_type': 'OPEN',\n",
       " 'base_type': 'vqa',\n",
       " 'content_type': 'Position',\n",
       " 'triple': ['vhead', '_', '_'],\n",
       " 'qid': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = dict()\n",
    "ids_dict = dict()\n",
    "for i in train_data:\n",
    "    organ = i['location']\n",
    "    id = i['qid']\n",
    "    if num_dict.__contains__(organ):\n",
    "        num_dict[organ] += 1\n",
    "    else:\n",
    "        num_dict[organ] = 1\n",
    "    \n",
    "    if ids_dict.__contains__(organ):\n",
    "        ids_dict[organ].append(id)\n",
    "    else:\n",
    "        ids_dict[organ] = [id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abdomen': 3041,\n",
       " 'Lung': 3406,\n",
       " 'Chest_heart': 187,\n",
       " 'Chest_lung': 283,\n",
       " 'Brain_Tissue': 1394,\n",
       " 'Brain_Face': 250,\n",
       " 'Brain': 543,\n",
       " 'Neck': 264,\n",
       " 'Chest_mediastinal': 33,\n",
       " 'Pelvic Cavity': 434}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lungs = []\n",
    "for _id in ids_dict['Lung']:\n",
    "    if seg_train_dict.__contains__(_id):\n",
    "        train_data_lungs.append(seg_train_dict[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_lungs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/aofei/hallucination/Slake/data/training_masks_top8_lung.json', 'w') as json_file:\n",
    "    json.dump(train_data_lungs, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lungs = []\n",
    "for _id in ids_dict['Abdomen']:\n",
    "    if seg_train_dict.__contains__(_id):\n",
    "        train_data_lungs.append(seg_train_dict[_id])\n",
    "with open('/data/aofei/hallucination/Slake/data/training_masks_top8_abd.json', 'w') as json_file:\n",
    "    json.dump(train_data_lungs, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_with_indices(image_name='xmlab1/source.jpg', indices_list=[3,0,2,1], fig_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_rad_data[-20:-10]:\n",
    "    image_name = i['image_name']\n",
    "    indices_list = i['bbox_indices']\n",
    "    visualize_with_indices(image_name=image_name, indices_list=indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_with_indices(image_name='synpic28602.jpg', indices_list=[3, 8, 9], fig_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_with_indices(image_name, indices_list:list = None, indices:tuple = None, width_threshold=260, fig_width=6):\n",
    "    image_path = os.path.join(\"/data/aofei/hallucination/Slake/imgs\", image_name)\n",
    "    masks = masks_image_dict[image_path]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_with_boxes = image_rgb.copy()\n",
    "\n",
    "    # Create an empty mask with the same size as the image\n",
    "    combined_mask = np.zeros_like(image_rgb, dtype=np.uint8)\n",
    "\n",
    "    # Loop over all segmentation results\n",
    "    selected_masks = []\n",
    "    if indices is not None:\n",
    "        selected_masks = masks[indices[0]:indices[1]]\n",
    "    elif indices_list is not None:\n",
    "        for i in indices_list:\n",
    "            selected_masks.append(masks[i])\n",
    "\n",
    "    for seg in selected_masks:\n",
    "        # Unpack bbox (bounding box)\n",
    "        x, y, w, h = seg['bbox']\n",
    "        if w >= width_threshold:\n",
    "            continue\n",
    "\n",
    "        # Draw the bounding box (in red)\n",
    "        cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "\n",
    "        # Extract and resize the segmentation mask\n",
    "        mask = seg['segmentation'].astype(np.uint8)\n",
    "        mask = cv2.resize(mask, (image_rgb.shape[1], image_rgb.shape[0]))  # Resize to fit the image\n",
    "\n",
    "        # Add mask to combined mask (use a different color for each mask if desired)\n",
    "        color_mask = np.zeros_like(image_rgb)\n",
    "        color_mask[mask == 1] = [0, 255, 0]  # Green mask for the segment\n",
    "        combined_mask = np.maximum(combined_mask, color_mask)\n",
    "\n",
    "    # Blend the original image with the combined mask once\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    image_with_masks = cv2.addWeighted(image_with_boxes, 1 - alpha, combined_mask, alpha, 0)\n",
    "\n",
    "    # Display the image with bounding boxes and masks\n",
    "    plt.figure(figsize=(fig_width, fig_width))\n",
    "    plt.imshow(image_with_masks)\n",
    "    plt.axis('off')  # Turn off axis for clean visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
